{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XotQU1Ootpi6",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Bayesian logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jaxtyping import Array, Float\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from coinem.model import AbstractModel\n",
    "from coinem.dataset import Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider two variants of the logistic regression model considered by Liu and Wang (2016). One where the parameters of the Gamma distribution on the prior of $\\alpha$ are fixed, and the other where we learn them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability.substrates.jax.distributions as tfd\n",
    "\n",
    "@dataclass\n",
    "class LogisticRegressionLearnTheta(AbstractModel):\n",
    "    \"\"\"Base class for p(θ, x).\"\"\"\n",
    "    \n",
    "    def log_prob(self, latent: Float[Array, \"D 1\"], theta: Float[Array, \"Q\"], data: Dataset) -> Float[Array, \"\"]:\n",
    "        \"\"\"Compute gradient of the objective function at x.\n",
    "\n",
    "        Args:\n",
    "            latent (Float[Array, \"D\"]): Input weights of shape (D,).\n",
    "            theta (Float[Array, \"Q\"]): Parameters of shape (Q,).\n",
    "\n",
    "        Returns:\n",
    "            Float[Array, \"\"]: log-probability of the data.\n",
    "        \"\"\"\n",
    "        alpha = jnp.exp(latent[0])\n",
    "        beta = latent[1:]\n",
    "\n",
    "\n",
    "        # likelihood\n",
    "        z = jnp.matmul(data.X, beta)\n",
    "        log_lik = tfd.Bernoulli(logits=z.squeeze()).log_prob(data.y.squeeze()).sum()\n",
    "        \n",
    "\n",
    "        # Compute linear predictor.\n",
    "        z = jnp.matmul(data.X, beta)\n",
    "\n",
    "        # Prior\n",
    "        log_prior = tfd.Normal(loc=0.0, scale=1.0/jnp.sqrt(alpha)).log_prob(beta).sum().squeeze()\n",
    "        log_prior_alpha = tfd.Gamma(jnp.exp(theta[0]).squeeze(), rate=jnp.exp(theta[1])).log_prob(alpha).sum().squeeze()\n",
    "\n",
    "\n",
    "        # Compute log-probability.\n",
    "        return (log_lik + log_prior + log_prior_alpha).squeeze()\n",
    "    \n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LogisticRegressionFixedTheta(AbstractModel):\n",
    "    \"\"\"Base class for p(θ, x).\"\"\"\n",
    "    \n",
    "    def log_prob(self, latent: Float[Array, \"D 1\"], theta: Float[Array, \"Q\"], data: Dataset) -> Float[Array, \"\"]:\n",
    "        \"\"\"Compute gradient of the objective function at x.\n",
    "\n",
    "        Args:\n",
    "            latent (Float[Array, \"D\"]): Input weights of shape (D,).\n",
    "            theta (Float[Array, \"Q\"]): Parameters of shape (Q,).\n",
    "\n",
    "        Returns:\n",
    "            Float[Array, \"\"]: log-probability of the data.\n",
    "        \"\"\"\n",
    "        alpha = jnp.exp(latent[0])\n",
    "        beta = latent[1:]\n",
    "\n",
    "\n",
    "        # likelihood\n",
    "        z = jnp.matmul(data.X, beta)\n",
    "        log_lik = tfd.Bernoulli(logits=z.squeeze()).log_prob(data.y.squeeze()).sum()\n",
    "        \n",
    "\n",
    "        # Compute linear predictor.\n",
    "        z = jnp.matmul(data.X, beta)\n",
    "\n",
    "        # Prior\n",
    "        log_prior = tfd.Normal(loc=0.0, scale=1.0/jnp.sqrt(alpha)).log_prob(beta).sum().squeeze()\n",
    "        log_prior_alpha = tfd.Gamma(concentration=1.0, rate=0.01).log_prob(alpha).sum().squeeze()\n",
    "\n",
    "\n",
    "        # Compute log-probability.\n",
    "        return (log_lik + log_prior + log_prior_alpha).squeeze()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the models on covertype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = scipy.io.loadmat('data/covertype.mat')\n",
    "    \n",
    "X_input = data['covtype'][:, 1:]\n",
    "y_input = data['covtype'][:, 0]\n",
    "y_input[y_input == 2] = 0\n",
    "\n",
    "N = X_input.shape[0]\n",
    "X_input = np.hstack([X_input, np.ones([N, 1])])\n",
    "\n",
    "# split the dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_input, y_input, test_size=0.2, random_state=42)\n",
    "\n",
    "D = Dataset(jnp.array(X_train), jnp.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 56)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "a0, b0 = 1, 0.01\n",
    "th0 = jnp.array([jnp.log(a0), jnp.log(b0)])\n",
    "\n",
    "np.random.seed(42)\n",
    "    \n",
    "# initialization\n",
    "N = 100  # number of particles\n",
    "d = D.X.shape[-1]\n",
    "X0 = np.zeros([N, 1 + d])\n",
    "alpha0 = np.random.gamma(a0, b0, N); \n",
    "for i in range(N):\n",
    "    X0[i, :] = np.hstack([np.log(alpha0[i]), np.random.normal(0, np.sqrt(1 / alpha0[i]), d)])\n",
    "\n",
    "X0 = jnp.array(X0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "5WG-_AlQR_S_"
   },
   "outputs": [],
   "source": [
    "from coinem.zoo import coin_svgd\n",
    "from jax import vmap\n",
    "import jax.random as jr\n",
    "\n",
    "fixed_theta = LogisticRegressionFixedTheta()\n",
    "learn_theta = LogisticRegressionLearnTheta()\n",
    "\n",
    "# Set approximation parameters:\n",
    "K = 1000  # Number of steps.\n",
    "\n",
    "\n",
    "# Run SVGD:\n",
    "x_coin_fixed, theta_coin_fixed = coin_svgd(fixed_theta, D, X0, th0, K, batch_size=100)\n",
    "x_coin_learn, theta_coin_learn = coin_svgd(learn_theta, D, X0, th0, K, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coin Fixed AUC: 0.8040118644813954\n",
      "Coin Learn AUC: 0.8049609884562804\n"
     ]
    }
   ],
   "source": [
    "# Compute AUC:\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def predict_prob(test_inputs, latent):\n",
    "    \"\"\"Returns label maximizing the approximate posterior predictive \n",
    "    distribution defined by the cloud X, vectorized over feature vectors f.\n",
    "    \"\"\"\n",
    "    s = vmap(lambda x: tfd.Bernoulli(logits=jnp.matmul(test_inputs, x[1:])).mean())(latent).mean(0)\n",
    "    return s\n",
    "\n",
    "coin_fixed_auc = roc_auc_score(y_test, predict_prob(X_test, x_coin_fixed[-1]))\n",
    "coin_learn_auc = roc_auc_score(y_test, predict_prob(X_test, x_coin_learn[-1]))\n",
    "\n",
    "print(f\"Coin Fixed AUC: {coin_fixed_auc}\")\n",
    "print(f\"Coin Learn AUC: {coin_learn_auc}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
