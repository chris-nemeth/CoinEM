{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jaxtyping import Array, Float\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability.substrates.jax.distributions as tfd\n",
    "\n",
    "from coinem.model import AbstractModel\n",
    "from coinem.dataset import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We employ standard Bayesian logistic regression with Gaussian priors. That is, we assume that the datapoints’ labels are conditionally independent given the features $f$ and regression weights $x \\in \\mathbb{R}^{Dy:=9}$, each label with Bernoulli law and mean $s(f^T x)$, where $s(z) := e^z/(1 + e^z)$ denotes the standard logistic function; and we assign the prior $\\mathcal{N}(x; \\theta I_{D_y}, 5I_{D_y})$ to the weights $x$, where $\\theta$ denotes the (scalar) parameter to be estimated. The model’s density is given by:\n",
    "\n",
    "$$ p_\\theta(x, y) = \\mathcal{N}(x; \\theta I_{D_y}, 5I_{D_y}) \\prod_{f, l} s(f^{T} x)^{l} \\left[ 1 - s(f^{T} x) \\right]^{1-l}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model in code:\n",
    "To create our model we just inherit from the provided `AbstractModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Array, Float, PyTree\n",
    "import tensorflow_probability.substrates.jax.bijectors as tfb\n",
    "\n",
    "@dataclass\n",
    "class LogisticRegression(AbstractModel):\n",
    "    \"\"\"Base class for p(θ, x).\"\"\"\n",
    "    \n",
    "    def log_prob(self, latent: Float[Array, \"D 1\"], theta: Float[Array, \"Q\"], data: Dataset) -> Float[Array, \"\"]:\n",
    "        \"\"\"Compute gradient of the objective function at x.\n",
    "\n",
    "        Args:\n",
    "            latent (Float[Array, \"D\"]): Input weights of shape (D,).\n",
    "            theta (Float[Array, \"Q\"]): Parameters of shape (Q,).\n",
    "\n",
    "        Returns:\n",
    "            Float[Array, \"\"]: log-probability of the data.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute linear predictor.\n",
    "        z = jnp.matmul(data.X, latent)\n",
    "\n",
    "        # likelihood\n",
    "        log_lik = tfd.Bernoulli(logits=z).log_prob(data.y.squeeze()).sum()\n",
    "\n",
    "        # Prior\n",
    "        log_prior = tfd.Normal(loc=theta, scale=5.0).log_prob(latent).sum().squeeze()\n",
    "\n",
    "        # Compute log-probability.\n",
    "        return log_lik + log_prior\n",
    "    \n",
    "    def optimal_theta(self, latent_particles: PyTree[Float[Array, \"N D *\"]]) -> PyTree[Float[Array, \"Q *\"]]:\n",
    "        return latent_particles.mean().reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset:\n",
    "\n",
    "We use the Wisconsin Breast Cancer dataset Y (Wolberg and Mangasarian, 1990), created by Dr. William H. Wolberg at the University of Wisconsin Hospitals, and freely available at\n",
    "https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original).\n",
    "It contains 683 datapoints5 each with nine features f ∈ R9 extracted from a digitized image of a fine needle aspirate of a breast mass and an accompanying label l indicating whether the mass is benign (l = 0) or malign (l = 1). We normalize the features so that each has mean zero and unit standard deviation across the dataset. We split the dataset into 80/20 training and testing sets, Ytrain and Ytest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS and wget to load dataset.\n",
    "import os\n",
    "import wget\n",
    "import numpy as np\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Fetch dataset from repository:\n",
    "wget.download('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data')\n",
    "\n",
    "# Load dataset:\n",
    "dataset = np.loadtxt('breast-cancer-wisconsin.data', dtype=str, delimiter=',')\n",
    "\n",
    "# Delete local copy of dataset to avoid duplicates with multiple notebook runs:\n",
    "os.remove('breast-cancer-wisconsin.data')\n",
    "\n",
    "# Remove datapoints with missing attributes and change dtype to float:\n",
    "dataset = dataset[~(dataset == '?').any(axis=1), :].astype(float)\n",
    "\n",
    "# Extract features and labels, and normalize features:\n",
    "features = np.array(dataset[:, 1:10] - dataset[:, 1:10].mean(0))\n",
    "features = features/features.std(0)\n",
    "labels = np.array([(dataset[:, 10]-2)/2]).transpose()\n",
    "\n",
    "# Split data into 80/20 training and testing sets:\n",
    "from sklearn.model_selection import train_test_split\n",
    "ftrain, ftest, ltrain, ltest = train_test_split(features, labels, test_size=0.2, \n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit tests:\n",
    "We test the score of the latent function and average log-probability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model:\n",
    "model = LogisticRegression()\n",
    "data = Dataset(ftrain, ltrain)\n",
    "\n",
    "# Unit tests:\n",
    "N = 10  # Number of particles.\n",
    "D = 9  # Dimension of latent space.\n",
    "\n",
    "theta = jnp.array([0.0]) # Parameters of the model.\n",
    "latent_particles = jnp.zeros((N, D)) # Latent particles.\n",
    "\n",
    "f = ftrain # Features\n",
    "y = ltrain # Labels\n",
    "\n",
    "s = 1.0/(1.0+jnp.exp(-jnp.matmul(f, latent_particles.T)))\n",
    "score_latent_particles = (jnp.matmul((y-s).T, f).T - (latent_particles.T)/5).T\n",
    "average_score_theta = jnp.array([[2 * (latent_particles.T).sum(0).mean()/5]])\n",
    "\n",
    "assert jnp.allclose(model.score_latent_particles(latent_particles, theta, data), score_latent_particles) # Gradient of log p(y|x) wrt latent particles\n",
    "assert jnp.allclose(model.average_score_theta(latent_particles, theta, data), average_score_theta) # Average gradient of log p(y|x) wrt theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to run all of the methods. First we'll investigate the parameter estimates and the corresponding posteriors obtained each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coinem.zoo import coin_svgd, pgd, ada_pgd, ada_svgd, soul, adam_svgd, standard_svgd\n",
    "from coinem.marginal_zoo import marginal_coin_svgd, marginal_pgd\n",
    "\n",
    "ftrain, ftest, ltrain, ltest = train_test_split(features, labels, test_size=0.2,random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "data = Dataset(ftrain, ltrain)\n",
    "    \n",
    "import jax.random as jr\n",
    "key = jr.PRNGKey(42)\n",
    "\n",
    "K = 800  # Number of steps.\n",
    "N = 100 # Number of particles.\n",
    "\n",
    "th0 = jnp.array([0.0])  # Parameter estimate.\n",
    "X0 = jr.normal(key, (N, D))  # Particle cloud.\n",
    "\n",
    "X_coin, th_coin = coin_svgd(model, data, X0, th0, K)\n",
    "X_svgd, th_svgd = standard_svgd(model, data, X0, th0, K, latent_step_size=2e-1, theta_step_size=2e-1)\n",
    "X_pgd, th_pgd = pgd(model, data, X0, th0, K, latent_step_size=2e-2, theta_step_size=2e-2)\n",
    "X_soul, th_soul = soul(model, data, X0, th0, K, latent_step_size=2e-2, theta_step_size=2e-2)\n",
    "X_pmgd, th_pmgd = marginal_pgd(model, data, X0, th0, K, latent_step_size=2e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Estimates [Fig. 3(a), Fig. 10, Fig.11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll plot the parameter estimates vs the iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "plt.plot(th_coin[:, 0].squeeze(), label='Coin EM', color=\"C0\", linewidth=3, zorder=6, linestyle=\"solid\")\n",
    "plt.plot(th_svgd[:, 0].squeeze(), label='SVGD EM', color=\"C2\", linewidth=3, zorder=5, linestyle='dashed')\n",
    "plt.plot(th_pgd[:, 0].squeeze(), label='PGD', linewidth=3, color=\"C1\", zorder=3, linestyle='dotted')\n",
    "plt.plot(th_soul[:, 0].squeeze(), label='SOUL', linewidth=3, color=\"C3\", zorder=4, linestyle='dashdot')\n",
    "plt.plot(th_pmgd[:, 0].squeeze(), label='PMGD', linewidth=3, color=\"C4\", zorder=1, linestyle=(5, (10, 3)))\n",
    "\n",
    "plt.legend(loc='upper right',prop={'size': 16})\n",
    "plt.xlim([-K/100, K])\n",
    "plt.ylim(0,2)\n",
    "plt.xlabel(\"Iterations\", fontsize=20)\n",
    "plt.ylabel(\"Parameter Estimate\", fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(color='whitesmoke')\n",
    "ax.set_axisbelow(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Estimates [Fig. 3(b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can take a look at the corresponding posterior density estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract final particle clouds X^{1:N}_K:\n",
    "q_pgd = X_pgd[-1, :, :]\n",
    "q_soul = X_soul[-1, :, :]\n",
    "q_coin = X_coin[-1, :, :]\n",
    "q_pmgd = X_pmgd[-1, :, :]\n",
    "q_svgd = X_svgd[-1, :, :]\n",
    "\n",
    "# Generate and plot KDEs:\n",
    "from scipy import stats  # stats to generate KDEs.\n",
    "fig, axs = plt.subplots(2,2, sharey=True)\n",
    "axs = axs.flatten()\n",
    "dim = 2\n",
    "\n",
    "for i in range(4):\n",
    "    # Generate KDEs for ith entry of the final particle cloud X^{1:N}_K:\n",
    "    kde_min = np.min([q_pgd[:, i], q_pmgd[:, i], q_soul[:, i], q_coin[:, i], q_svgd[:,i]])\n",
    "    kde_max = np.max([q_pgd[:, i], q_pmgd[:, i], q_soul[:, i], q_coin[:, i], q_svgd[:, i]])\n",
    "    xaxis = np.linspace(kde_min, kde_max, num=100)\n",
    "\n",
    "    kde_coin = stats.gaussian_kde(q_coin[:, i])(xaxis)\n",
    "    kde_svgd = stats.gaussian_kde(q_svgd[:, i])(xaxis)\n",
    "    kde_pgd = stats.gaussian_kde(q_pgd[:, i])(xaxis)\n",
    "    kde_pmgd = stats.gaussian_kde(q_pmgd[:, i])(xaxis)\n",
    "    kde_soul = stats.gaussian_kde(q_soul[:, i])(xaxis)\n",
    "    \n",
    "    # Plot KDEs:\n",
    "    axs[i].plot(xaxis, kde_coin, label='Coin EM', linewidth=2, color=\"C0\")\n",
    "    axs[i].plot(xaxis, kde_svgd, label='SVGD EM', linewidth=2, color=\"C2\")\n",
    "    axs[i].plot(xaxis, kde_pgd, label='PGD', linewidth=2, zorder=5, color=\"C1\")\n",
    "    axs[i].plot(xaxis, kde_pmgd, label='PMGD', linewidth=2, color=\"C3\")\n",
    "    axs[i].plot(xaxis, kde_soul, label='SOUL', linewidth=2, color=\"C4\")\n",
    "    axs[i].set_ylim([0, 1.1*np.max([kde_pgd, kde_pmgd, kde_svgd, kde_soul, kde_coin])])\n",
    "    axs[i].set_xlim([kde_min-0.05, kde_max+0.05])\n",
    "    axs[i].tick_params(labelsize=16)\n",
    "    if i == 1:\n",
    "        axs[i].legend(prop={'size': 14})\n",
    "    axs[i].grid(color='whitesmoke')\n",
    "    \n",
    "plt.subplots_adjust(wspace=.05, hspace=.18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's take a look at the predictive performance of each method for a range of learning rates, and different numbers of particles. In particular, we'll run each grid over a fine grid of learning rates, for N={5,20,100} particles, and compute the test-error and log pointwise predictive density. Each experiment, we'll use a different random train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coinem.zoo import coin_svgd, pgd, soul, adam_svgd\n",
    "from coinem.marginal_zoo import marginal_pgd\n",
    "\n",
    "import jax.random as jr\n",
    "from jax import vmap\n",
    "\n",
    "def predict_prob(f, X):\n",
    "    \"\"\"Returns label maximizing the approximate posterior predictive \n",
    "    distribution defined by the cloud X, vectorized over feature vectors f.\n",
    "    \"\"\"\n",
    "    s = vmap(lambda x: tfd.Bernoulli(logits=jnp.matmul(f, x)).mean())(X).mean(0)\n",
    "    return s\n",
    "\n",
    "def test_error(f, l, X):\n",
    "    \"\"\"Returns fraction of misclassified test points.\"\"\"\n",
    "\n",
    "    s = predict_prob(f, X)\n",
    "    pred = np.zeros((f[:, 0].size, 1))\n",
    "    pred[s >= 1/2] = 1\n",
    "    return np.abs(l - pred).mean()\n",
    "\n",
    "\n",
    "def lppd(f, l, X):\n",
    "    \"\"\"Returns log pointwise predictive density.\"\"\"\n",
    "    s = vmap(lambda x: tfd.Bernoulli(logits=jnp.matmul(f, x)).log_prob(l.squeeze()) )(X).mean()\n",
    "    return s\n",
    "\n",
    "# Random seeds for reproducibility:\n",
    "seeds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Learning rates:\n",
    "step_sizes = jnp.logspace(-10, 2, num=50) \n",
    "\n",
    "K = 400  # Number of steps.\n",
    "N = 100  # Number of particles.\n",
    "    \n",
    "coin_errors = []\n",
    "adam_errors = []\n",
    "pgd_errors = []\n",
    "soul_errors = []\n",
    "pmgd_errors = []\n",
    "\n",
    "coin_lppd = []\n",
    "adam_lppd = []\n",
    "pgd_lppd = []\n",
    "soul_lppd = []\n",
    "pmgd_lppd = []\n",
    "\n",
    "\n",
    "for seed in seeds:\n",
    "    print(\"Seed: \" + str(seed+1) + \"/\" + str(len(seeds)))\n",
    "\n",
    "    ftrain, ftest, ltrain, ltest = train_test_split(features, labels, test_size=0.2,random_state=seed)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    data = Dataset(ftrain, ltrain)\n",
    "\n",
    "    key = jr.PRNGKey(seed)\n",
    "\n",
    "    key, subkey = jr.split(key)\n",
    "\n",
    "    th0 = jnp.array([0.0])  # Parameter estimate.\n",
    "    X0 = jr.normal(subkey, (N, D))  # Particle cloud.\n",
    "\n",
    "    x_coin, th_coin  = coin_svgd(model, data, X0, th0, K)\n",
    "\n",
    "    coin_errors.append(test_error(ftest, ltest, x_coin[-1]))\n",
    "    coin_lppd.append(lppd(ftest, ltest, x_coin[-1]))\n",
    "\n",
    "\n",
    "    #for theta_step, latent_step in zip(theta_step_sizes, latent_step_sizes):\n",
    "    for jj, step in enumerate(step_sizes):\n",
    "        print(\"LR: \" + str(jj+1) + \"/\" + str(len(step_sizes)))\n",
    "        theta_step = step\n",
    "        latent_step = step\n",
    "\n",
    "        key_adam, key_pmgd = jr.split(key)\n",
    "        key_pgd, key_soul = jr.split(key)\n",
    "\n",
    "        x_adam, th_adam = adam_svgd(model, data, X0, th0, K, theta_step_size=theta_step, latent_step_size=latent_step, key=key_adam)\n",
    "        X_pgd, th_pgd = pgd(model, data, X0, th0, K, theta_step_size=theta_step, latent_step_size=latent_step, key=key_pgd)\n",
    "        X_soul, th_soul = soul(model, data, X0, th0, K, theta_step_size=theta_step, latent_step_size=latent_step, key=key_soul)\n",
    "        X_pmgd, th_pmgd = marginal_pgd(model, data, X0, th0, K, latent_step_size=latent_step, key=key_pmgd)\n",
    "\n",
    "        adam_errors.append(test_error(ftest, ltest, x_adam[-1]))\n",
    "        pgd_errors.append(test_error(ftest, ltest, X_pgd[-1]))\n",
    "        soul_errors.append(test_error(ftest, ltest, X_soul[-1]))\n",
    "        pmgd_errors.append(test_error(ftest, ltest, X_pmgd[-1]))\n",
    "\n",
    "        adam_lppd.append(lppd(ftest, ltest, x_adam[-1]))\n",
    "        pgd_lppd.append(lppd(ftest, ltest, X_pgd[-1]))\n",
    "        soul_lppd.append(lppd(ftest, ltest, X_soul[-1]))\n",
    "        pmgd_lppd.append(lppd(ftest, ltest, X_pmgd[-1]))\n",
    "\n",
    "# Reshape results:\n",
    "coin_errors = jnp.array(coin_errors)\n",
    "adam_errors = jnp.array(adam_errors).reshape(-1, step_sizes.shape[0])\n",
    "pgd_errors = jnp.array(pgd_errors).reshape(-1, step_sizes.shape[0])\n",
    "soul_errors = jnp.array(soul_errors).reshape(-1, step_sizes.shape[0])\n",
    "pmgd_errors = jnp.array(pmgd_errors).reshape(-1, step_sizes.shape[0])\n",
    "\n",
    "coin_lppd = jnp.array(coin_lppd)\n",
    "adam_lppd = jnp.array(adam_lppd).reshape(-1, step_sizes.shape[0])\n",
    "pgd_lppd = jnp.array(pgd_lppd).reshape(-1, step_sizes.shape[0])\n",
    "soul_lppd = jnp.array(soul_lppd).reshape(-1, step_sizes.shape[0])\n",
    "pmgd_lppd = jnp.array(pmgd_lppd).reshape(-1, step_sizes.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Error [Fig. 3(c), Fig. 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axhline(coin_errors.mean(), color='C0', label='Coin EM')\n",
    "plt.axhspan(coin_errors.mean() - coin_errors.std()/jnp.sqrt(10), coin_errors.mean() + coin_errors.std()/jnp.sqrt(10), alpha=0.1, color='C0')\n",
    "\n",
    "plt.plot(step_sizes, adam_errors.mean(0), marker=\".\",label='SVGD EM', color='C2', linestyle=\"dashed\")\n",
    "plt.fill_between(step_sizes, adam_errors.mean(0) - adam_errors.std(0)/jnp.sqrt(10), adam_errors.mean(0) + adam_errors.std(0)/jnp.sqrt(10), alpha=0.1, color='C2')\n",
    "\n",
    "plt.plot(step_sizes, pgd_errors.mean(0), marker='.', label='PGD', color='C1', linestyle='dotted', zorder=5)\n",
    "plt.fill_between(step_sizes, pgd_errors.mean(0) - pgd_errors.std(0)/jnp.sqrt(10), pgd_errors.mean(0) + pgd_errors.std(0)/jnp.sqrt(10), alpha=0.1 , color='C1', zorder=5)\n",
    "\n",
    "plt.plot(step_sizes, soul_errors.mean(0), marker='.', label='SOUL' , color='C3', linestyle='dashdot', zorder=2)\n",
    "plt.fill_between(step_sizes, soul_errors.mean(0) - soul_errors.std(0)/jnp.sqrt(10), soul_errors.mean(0) + soul_errors.std(0)/jnp.sqrt(10), alpha=0.1 , color='C3', zorder=2)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate', fontsize=20)\n",
    "plt.ylabel('Test Error', fontsize=20)\n",
    "plt.legend(prop={'size': 16}, loc='upper right')\n",
    "plt.grid(color='whitesmoke')\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.ylim(0,0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Pointwise Predictive Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axhline(coin_lppd.mean(), color='C0', label='Coin EM')\n",
    "plt.axhspan(coin_lppd.mean() - coin_lppd.std()/jnp.sqrt(10), coin_lppd.mean() + coin_lppd.std()/jnp.sqrt(10), alpha=0.1, color='C0')\n",
    "\n",
    "plt.plot(step_sizes, adam_lppd.mean(0), marker=\".\",label='SVGD EM', color='C2', linestyle=\"dashed\")\n",
    "plt.fill_between(step_sizes, adam_lppd.mean(0) - adam_lppd.std(0)/jnp.sqrt(10), adam_lppd.mean(0) + adam_lppd.std(0)/jnp.sqrt(10), alpha=0.1, color='C2')\n",
    "\n",
    "plt.plot(step_sizes, pgd_lppd.mean(0), marker='.', label='PGD', color='C1', linestyle='dotted')\n",
    "plt.fill_between(step_sizes, pgd_lppd.mean(0) - pgd_lppd.std(0)/jnp.sqrt(10), pgd_lppd.mean(0) + pgd_lppd.std(0)/jnp.sqrt(10), alpha=0.1 , color='C1')\n",
    "\n",
    "plt.plot(step_sizes, soul_lppd.mean(0), marker='.', label='SOUL' , color='C3', linestyle='dashdot')\n",
    "plt.fill_between(step_sizes, soul_lppd.mean(0) - soul_lppd.std(0)/jnp.sqrt(10), soul_lppd.mean(0) + soul_lppd.std(0)/jnp.sqrt(10), alpha=0.1 , color='C3')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate', fontsize=20)\n",
    "plt.ylabel('Log Predictive Density', fontsize=20)\n",
    "plt.legend(prop={'size': 16}, loc='lower left')\n",
    "plt.ylim(-3,0)\n",
    "plt.grid(color='whitesmoke')\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gcsX2yOfakVA",
    "PXeTOIZy-tZA",
    "ElXW-4d9B9nc"
   ],
   "include_colab_link": true,
   "name": "Toy Hierarchical Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_m1",
   "language": "python",
   "name": "venv_m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
