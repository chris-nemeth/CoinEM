{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Hierarchical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jaxtyping import Array, Float\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability.substrates.jax.distributions as tfd\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "from scipy.stats import bootstrap\n",
    "\n",
    "from coinem.model import AbstractModel\n",
    "from coinem.dataset import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We consider generating data $y$ from a hierarchical Gaussian model with real-valued hyperparameter $\\theta = 1$:\n",
    "\n",
    "$$ p_\\theta(x, y) = \\mathcal{N}(y; x, I_{D_y}) \\mathcal{N}(x; \\theta I_{D_y}, I_{D_y}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model in code:\n",
    "To create our model we just inherit from the provided `AbstractModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Array, Float, PyTree\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HierarchicalModel(AbstractModel):\n",
    "    \"\"\"Base class for p(Î¸, x).\"\"\"\n",
    "    \n",
    "    def log_prob(self, latent: Float[Array, \"D\"], theta: Float[Array, \"Q\"], data: Dataset) -> Float[Array, \"\"]:\n",
    "        \"\"\"Compute gradient of the objective function at x.\n",
    "\n",
    "        Args:\n",
    "            latent (Float[Array, \"D\"]): Input weights of shape (D,).\n",
    "            theta (Float[Array, \"Q\"]): Parameters of shape (Q,).\n",
    "\n",
    "        Returns:\n",
    "            Float[Array, \"\"]: log-probability of the data.\n",
    "        \"\"\"\n",
    "        log_prob_y_given_x = tfd.MultivariateNormalDiag(jnp.atleast_1d(latent.squeeze())).log_prob(jnp.atleast_1d(data.y.squeeze())).sum() # \\mathcal{N}(y; x, I_{D_y}) \n",
    "        log_prob_x_given_theta = tfd.Normal(theta, scale=1.0).log_prob(jnp.atleast_1d(latent.squeeze())).sum() # \\mathcal{N}(x; \\theta I_{D_y}, I_{D_y})\n",
    "\n",
    "        # Compute log-probability.\n",
    "        return (log_prob_y_given_x + log_prob_x_given_theta).squeeze() # log p(y|x) + log p(x|theta)\n",
    "    \n",
    "    def optimal_theta(self, latent_particles: PyTree[Float[Array, \"N D *\"]]) -> PyTree[Float[Array, \"Q *\"]]:\n",
    "        return latent_particles.mean().reshape(-1) # WARNING: This assumes a single dimension for the \"x\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset:\n",
    "We draw one observation $y$ from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ny = 1\n",
    "Dy = 100\n",
    "key = jr.PRNGKey(42)\n",
    "theta = jnp.array([1.0])\n",
    "\n",
    "key_y, key_latent = jr.split(key)\n",
    "latent = theta + jr.normal(key_latent, (1, Dy))\n",
    "y = jr.normal(key_y, (Ny, Dy)) + latent\n",
    "\n",
    "theta_star = jnp.sum(y)/Dy\n",
    "\n",
    "posterior_mean = 0.5 * (y + theta*jnp.ones_like(y))\n",
    "posterior_mean_empirical = 0.5 * (y + theta_star * jnp.ones_like(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit tests:\n",
    "We test the score of the latent function and average log-probability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model:\n",
    "model = HierarchicalModel()\n",
    "data = Dataset(y=y)\n",
    "\n",
    "# Unit tests:\n",
    "assert jnp.allclose(model.score_latent(latent, theta, data), (y + theta - 2.0* latent)) # Gradient of log p(y|x) wrt x\n",
    "assert jnp.allclose(model.score_theta(latent, theta, data), Dy * (latent.mean() - theta)) # Gradient of log p(y|x) wrt theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM algorithm\n",
    "\n",
    "For this example we can find the EM algorithm in closed form, for the theta updates given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def em(model, data, theta, K):\n",
    "    \"\"\"Expectation Maximization Algorithm. Returns parameter estimates.\"\"\"\n",
    "\n",
    "    def em_step(theta, k):\n",
    "        new_theta = theta/2.0 + data.y.mean()/2.0\n",
    "        return new_theta, theta\n",
    "\n",
    "    _, thetas = jax.lax.scan(em_step, theta, jnp.arange(K))\n",
    "\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Parameter Estimates and Posterior Means [Fig. 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPnKONzlHagU"
   },
   "source": [
    "We first run all of the methods over a grid of learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coinem.zoo import coin_svgd, ada_svgd, soul, pgd, ada_pgd, standard_svgd\n",
    "from coinem.marginal_zoo import marginal_coin_svgd, marginal_ada_svgd, marginal_soul, marginal_pgd\n",
    "\n",
    "step_size_list = jnp.logspace(-5,3,50) # step sizes\n",
    "\n",
    "n_reps = 2 # number of repeats\n",
    "K = 500 # number of iterations\n",
    "N = 10 # number of particles\n",
    "\n",
    "mse_ada_pgd_all = np.zeros((n_reps, len(step_size_list)))\n",
    "mse_ada_svgd_all = np.zeros((n_reps, len(step_size_list)))\n",
    "mse_soul_all = np.zeros((n_reps, len(step_size_list)))\n",
    "mse_coin_all = np.zeros(n_reps)\n",
    "    \n",
    "for i, rep in enumerate(range(n_reps)):\n",
    "\n",
    "    print(\"Repetition: \" + str(i+1) + \"/\" + str(n_reps))\n",
    "\n",
    "    key = jr.PRNGKey(int(i))\n",
    "    Ny = 1\n",
    "    Dy = 100\n",
    "\n",
    "    theta = jnp.array([1.0])\n",
    "\n",
    "    key_y, key_latent = jr.split(key)\n",
    "    latent = theta + jr.normal(key_latent, (1, Dy))\n",
    "    y = jr.normal(key_y, (Ny, Dy)) + latent\n",
    "\n",
    "    theta_star = jnp.sum(y)/Dy\n",
    "\n",
    "    posterior_mean = 0.5 * (y + theta*jnp.ones_like(y))\n",
    "    posterior_mean_empirical = 0.5 * (y + theta_star * jnp.ones_like(y))\n",
    "\n",
    "    th0 = .1*jr.normal(key, (1,))\n",
    "    X0 = 1*jr.normal(key, (N, Dy)) \n",
    "\n",
    "    model = HierarchicalModel()\n",
    "    data = Dataset(y=y)\n",
    "\n",
    "    for j, step in enumerate(step_size_list):\n",
    "        if (j+1) % 10 == 0: \n",
    "            print(\":L: \" + str(j+1) + \"/\" + str(len(step_size_list)))\n",
    "        _, theta_ada_pgd = ada_pgd(model, data, X0, th0, K, latent_step_size = step, theta_step_size = step)\n",
    "        _, theta_ada_svgd = ada_svgd(model, data, X0, th0, K, theta_step_size = step, latent_step_size = step)\n",
    "        _, theta_soul = soul(model, data, X0, th0, K, theta_step_size = step, latent_step_size = step)\n",
    "        \n",
    "        mse_ada_pgd = (theta_ada_pgd[-1] - theta_star)**2\n",
    "        mse_ada_svgd = (theta_ada_svgd[-1] - theta_star)**2\n",
    "        mse_soul = (theta_soul[-1] - theta_star)**2\n",
    "\n",
    "        mse_ada_pgd_all[i,j] = mse_ada_pgd\n",
    "        mse_ada_svgd_all[i,j] = mse_ada_svgd\n",
    "        mse_soul_all[i,j] = mse_soul\n",
    "\n",
    "    _, theta_coin = coin_svgd(model, data, X0, th0, K)\n",
    "    mse_coin = (theta_coin[-1] - theta_star) ** 2\n",
    "    mse_coin_all[i] = mse_coin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) MSE vs Learning Rate [Fig. 1(a), Fig. 6(a), Fig. 7(a), Fig. 9(a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute confidence intervals\n",
    "data_ada_pgd = (mse_ada_pgd_all,)  # samples must be in a sequence\n",
    "res_ada_pgd = bootstrap(data_ada_pgd, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_ada_svgd = (mse_ada_svgd_all,) \n",
    "res_ada_svgd = bootstrap(data_ada_svgd, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_soul = (mse_soul_all,) \n",
    "res_soul = bootstrap(data_soul, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_coin = (mse_coin_all,) \n",
    "res_coin = bootstrap(data_coin, np.mean, confidence_level=0.9)\n",
    "\n",
    "plt.axhline(np.mean(mse_coin_all), color=\"C0\", label=\"Coin EM\")\n",
    "plt.fill_between(step_size_list, res_coin.confidence_interval[0], res_coin.confidence_interval[1], color=\"C0\", alpha=0.2, linewidth=3)\n",
    "\n",
    "plt.plot(step_size_list, np.mean(mse_ada_svgd_all,0), color=\"C2\", linestyle='dashed', label=\"SVGD EM\", linewidth=3)\n",
    "plt.fill_between(step_size_list, res_ada_svgd.confidence_interval[0], res_ada_svgd.confidence_interval[1], alpha=0.2, color=\"C2\")\n",
    "\n",
    "plt.plot(step_size_list, np.mean(mse_ada_pgd_all,0), color=\"C1\", linestyle='dotted', label=\"PGD\", linewidth=3)\n",
    "plt.fill_between(step_size_list, res_ada_pgd.confidence_interval[0], res_ada_pgd.confidence_interval[1], alpha=0.2, color=\"C1\")\n",
    "\n",
    "plt.plot(step_size_list, np.mean(mse_soul_all,0), color=\"C3\", linestyle='dashdot', label=\"Soul\", linewidth=3)\n",
    "plt.fill_between(step_size_list, res_soul.confidence_interval[0], res_soul.confidence_interval[1], alpha=0.2, color=\"C3\")\n",
    "\n",
    "plt.ylim(1e-16,10000)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\") \n",
    "plt.legend(loc='upper right',prop={'size': 20})\n",
    "plt.margins(x=0)\n",
    "plt.xlabel(\"Learning Rate\", fontsize=20)\n",
    "plt.ylabel(\"MSE (Theta Estimate)\", fontsize=20)\n",
    "plt.grid(color='whitesmoke')\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these results, we can compute an \"optimal\" step size for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_ada_pgd = np.mean(mse_ada_pgd_all,axis=0) # mean over reps\n",
    "mse_ada_svgd = np.mean(mse_ada_svgd_all,axis=0) # mean over reps\n",
    "mse_soul = np.mean(mse_soul_all, axis=0) # mean over reps\n",
    "\n",
    "lr_opt_ada_pgd_id = np.nanargmin(mse_ada_pgd)\n",
    "lr_opt_ada_svgd_id = np.nanargmin(mse_ada_svgd)\n",
    "lr_opt_soul_id = np.nanargmin(mse_soul)\n",
    "\n",
    "# optimal LR\n",
    "lr_opt_ada_pgd = step_size_list[lr_opt_ada_pgd_id]\n",
    "lr_opt_ada_svgd = step_size_list[lr_opt_ada_svgd_id]\n",
    "lr_opt_soul = step_size_list[lr_opt_soul_id]\n",
    "\n",
    "# smaller LR\n",
    "lr_small_ada_pgd = step_size_list[lr_opt_ada_pgd_id - 8]\n",
    "lr_small_ada_svgd = step_size_list[lr_opt_ada_svgd_id - 8]\n",
    "lr_small_soul = step_size_list[lr_opt_soul_id - 8]\n",
    "\n",
    "# big LR 'at edge of stability'\n",
    "lr_big_ada_pgd = step_size_list[np.isnan(mse_ada_pgd).argmax()] #step_size_list[lr_opt_ada_pgd_id + 1]\n",
    "lr_big_ada_svgd = 1e2 #step_size_list[lr_opt_ada_svgd_id + 1]\n",
    "lr_big_soul = step_size_list[np.isnan(mse_soul).argmax() - 1] #step_size_list[lr_opt_soul_id + 1]\n",
    "\n",
    "print(\"Optimal LR (Adagrad-PGD): \" + str(lr_opt_ada_pgd))\n",
    "print(\"Optimal LR (Adagrad-SVGD): \" + str(lr_opt_ada_svgd))\n",
    "print(\"Optimal LR (SOUL): \" + str(lr_opt_soul))\n",
    "\n",
    "print(\"Small LR (Adagrad-PGD): \" + str(lr_small_ada_pgd))\n",
    "print(\"Small LR (Adagrad-SVGD): \" + str(lr_small_ada_svgd))\n",
    "print(\"Small LR (SOUL): \" + str(lr_small_soul))\n",
    "\n",
    "print(\"Big LR (Adagrad-PGD): \" + str(lr_big_ada_pgd))\n",
    "print(\"Big LR (Adagrad-SVGD): \" + str(lr_big_ada_svgd))\n",
    "print(\"Big LR (SOUL): \" + str(lr_big_soul))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use these learning rates to run each method, now storing the results for all iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "kkmn65RaY5uk",
    "outputId": "ed8ff3bf-767e-4912-cb2a-27c41bf0f80f"
   },
   "outputs": [],
   "source": [
    "# run all methods for the optimal step sizes\n",
    "n_reps = 10\n",
    "\n",
    "# Optimal LR\n",
    "th_em_all, x_mean_em_all = np.zeros((n_reps, K)), np.zeros((n_reps, K, N, Dy))\n",
    "th_ada_pgd_all, x_ada_pgd_all = np.zeros((n_reps, K)), np.zeros((n_reps, K, N, Dy))\n",
    "th_ada_svgd_all, x_ada_svgd_all = np.zeros((n_reps, K)), np.zeros((n_reps, K, N, Dy))\n",
    "th_soul_all, x_soul_all = np.zeros((n_reps, K)), np.zeros((n_reps, K, N, Dy))\n",
    "\n",
    "mse_ada_pgd_all, mse_latent_ada_pgd_all = np.zeros((n_reps, K)), np.zeros((n_reps, K))\n",
    "mse_ada_svgd_all, mse_latent_ada_svgd_all = np.zeros((n_reps, K)), np.zeros((n_reps, K))\n",
    "mse_soul_all, mse_latent_soul_all = np.zeros((n_reps, K)), np.zeros((n_reps, K))\n",
    "mse_coin_all, mse_latent_coin_all = np.zeros((n_reps, K)), np.zeros((n_reps, K))\n",
    "\n",
    "# Small LR\n",
    "th_ada_pgd_all_small, x_ada_pgd_all_small = np.zeros((n_reps, K)), np.zeros((n_reps, K, N, Dy))\n",
    "th_ada_svgd_all_small, x_ada_svgd_all_small = np.zeros((n_reps, K)), np.zeros((n_reps, K, N, Dy))\n",
    "th_soul_all_small, x_soul_all_small = np.zeros((n_reps, K)), np.zeros((n_reps, K, N, Dy))\n",
    "\n",
    "mse_ada_pgd_all_small, mse_latent_ada_pgd_all_small = np.zeros((n_reps, K)), np.zeros((n_reps, K))\n",
    "mse_ada_svgd_all_small, mse_latent_ada_svgd_all_small = np.zeros((n_reps, K)), np.zeros((n_reps, K))\n",
    "mse_soul_all_small, mse_latent_soul_all_small = np.zeros((n_reps, K)), np.zeros((n_reps, K))\n",
    "mse_coin_all_small, mse_latent_coin_all_small = np.zeros((n_reps, K)), np.zeros((n_reps, K))\n",
    "\n",
    "# Big LR\n",
    "th_ada_pgd_all_big, x_ada_pgd_all_big = np.zeros((n_reps, K)), np.zeros((n_reps, K, N, Dy))\n",
    "th_ada_svgd_all_big, x_ada_svgd_all_big = np.zeros((n_reps, K)), np.zeros((n_reps, K, N, Dy))\n",
    "th_soul_all_big, x_soul_all_big = np.zeros((n_reps, K)), np.zeros((n_reps, K, N, Dy))\n",
    "\n",
    "mse_ada_pgd_all_big, mse_latent_ada_pgd_all_big = np.zeros((n_reps, K)), np.zeros((n_reps, K))\n",
    "mse_ada_svgd_all_big, mse_latent_ada_svgd_all_big = np.zeros((n_reps, K)), np.zeros((n_reps, K))\n",
    "mse_soul_all_big, mse_latent_soul_all_big = np.zeros((n_reps, K)), np.zeros((n_reps, K))\n",
    "mse_coin_all_big, mse_latent_coin_all_big = np.zeros((n_reps, K)), np.zeros((n_reps, K))\n",
    "\n",
    "# Coin \n",
    "th_coin_all, x_coin_all = np.zeros((n_reps, K)), np.zeros((n_reps, K, N, Dy))\n",
    "\n",
    "# maximisers\n",
    "theta_star_all = np.zeros((n_reps))\n",
    "posterior_mean_empirical_all = np.zeros((n_reps, Dy))\n",
    "\n",
    "for i, rep in enumerate(range(n_reps)):\n",
    "\n",
    "    print(\"Repetition: \" + str(i+1) + \"/\" + str(n_reps))\n",
    "\n",
    "    # Setup Model\n",
    "    key = jr.PRNGKey(int(i))\n",
    "    Ny = 1\n",
    "    Dy = 100\n",
    "\n",
    "    theta = jnp.array([1.0])\n",
    "\n",
    "    key_y, key_latent = jr.split(key)\n",
    "    latent = theta + jr.normal(key_latent, (1, Dy))\n",
    "    y = jr.normal(key_y, (Ny, Dy)) + latent\n",
    "\n",
    "    theta_star = jnp.sum(y)/Dy\n",
    "    theta_star_all[i] = theta_star\n",
    "\n",
    "    posterior_mean = 0.5 * (y + theta*jnp.ones_like(y))\n",
    "    posterior_mean_empirical = 0.5 * (y + theta_star * jnp.ones_like(y))\n",
    "\n",
    "    posterior_mean_empirical_all[i, :] = posterior_mean_empirical\n",
    "    \n",
    "    th0 = .1*jr.normal(key, (1,))\n",
    "    X0 = 1*jr.normal(key, (N, Dy)) \n",
    "\n",
    "    model = HierarchicalModel()\n",
    "    data = Dataset(y=y)\n",
    "\n",
    "    # EM\n",
    "    th_em = em(model, data, th0, K)\n",
    "    x_mean_em = (data.y.squeeze()  + th_em)\n",
    "    th_em_all[i, :] = th_em.squeeze()\n",
    "\n",
    "    # optimal LR\n",
    "    x_ada_pgd, th_ada_pgd = ada_pgd(model, data, X0, th0, K, latent_step_size=lr_opt_ada_pgd, theta_step_size=lr_opt_ada_pgd)\n",
    "    x_ada_svgd, th_ada_svgd = ada_svgd(model, data, X0, th0, K, theta_step_size=lr_opt_ada_svgd, latent_step_size=lr_opt_ada_svgd)\n",
    "    x_soul, th_soul = soul(model, data, X0, th0, K, theta_step_size=lr_opt_soul, latent_step_size=lr_opt_soul)\n",
    "\n",
    "    mse_ada_pgd = (th_ada_pgd - theta_star)**2\n",
    "    mse_ada_svgd = (th_ada_svgd - theta_star)**2\n",
    "    mse_soul = (th_soul - theta_star)**2\n",
    "\n",
    "    mse_latent_ada_pgd = ((x_ada_pgd.mean(axis=1) - posterior_mean_empirical[:,np.newaxis])**2).mean(axis=2)\n",
    "    mse_latent_ada_svgd = ((x_ada_svgd.mean(axis=1) - posterior_mean_empirical[:,np.newaxis])**2).mean(axis=2)\n",
    "    mse_latent_soul = ((x_soul.mean(axis=1) - posterior_mean_empirical[:,np.newaxis])**2).mean(axis=2)\n",
    "\n",
    "    x_ada_pgd_all[i, :, :, :], th_ada_pgd_all[i, :] = x_ada_pgd, th_ada_pgd.squeeze()\n",
    "    x_ada_svgd_all[i, :, :, :], th_ada_svgd_all[i, :] = x_ada_svgd, th_ada_svgd.squeeze()\n",
    "    x_soul_all[i, :, :, :], th_soul_all[i, :] = x_soul, th_soul.squeeze()\n",
    "\n",
    "    mse_ada_pgd_all[i,:] = mse_ada_pgd.squeeze()\n",
    "    mse_ada_svgd_all[i,:] = mse_ada_svgd.squeeze()\n",
    "    mse_soul_all[i,:] = mse_soul.squeeze()\n",
    "\n",
    "    mse_latent_ada_pgd_all[i,:] = mse_latent_ada_pgd.squeeze()\n",
    "    mse_latent_ada_svgd_all[i,:] = mse_latent_ada_svgd.squeeze()\n",
    "    mse_latent_soul_all[i,:] = mse_latent_soul.squeeze()\n",
    "\n",
    "    # small LR\n",
    "    x_ada_pgd_small, th_ada_pgd_small = ada_pgd(model, data, X0, th0, K, latent_step_size=lr_small_ada_pgd, theta_step_size=lr_small_ada_pgd)\n",
    "    x_ada_svgd_small, th_ada_svgd_small = ada_svgd(model, data, X0, th0, K, theta_step_size=lr_small_ada_svgd, latent_step_size=lr_small_ada_svgd)\n",
    "    x_soul_small, th_soul_small = soul(model, data, X0, th0, K, theta_step_size=lr_small_soul, latent_step_size=lr_small_soul)\n",
    "\n",
    "    mse_ada_pgd_small = (th_ada_pgd_small - theta_star)**2\n",
    "    mse_ada_svgd_small = (th_ada_svgd_small - theta_star)**2\n",
    "    mse_soul_small = (th_soul_small - theta_star)**2\n",
    "\n",
    "    mse_latent_ada_pgd_small = ((x_ada_pgd_small.mean(axis=1) - posterior_mean_empirical[:,np.newaxis])**2).mean(axis=2)\n",
    "    mse_latent_ada_svgd_small = ((x_ada_svgd_small.mean(axis=1) - posterior_mean_empirical[:,np.newaxis])**2).mean(axis=2)\n",
    "    mse_latent_soul_small = ((x_soul_small.mean(axis=1) - posterior_mean_empirical[:,np.newaxis])**2).mean(axis=2)\n",
    "\n",
    "    x_ada_pgd_all_small[i, :, :, :], th_ada_pgd_all_small[i, :] = x_ada_pgd_small, th_ada_pgd_small.squeeze()\n",
    "    x_ada_svgd_all_small[i, :, :, :], th_ada_svgd_all_small[i, :] = x_ada_svgd_small, th_ada_svgd_small.squeeze()\n",
    "    x_soul_all_small[i, :, :, :], th_soul_all_small[i, :] = x_soul_small, th_soul_small.squeeze()\n",
    "\n",
    "    mse_ada_pgd_all_small[i,:] = mse_ada_pgd_small.squeeze()\n",
    "    mse_ada_svgd_all_small[i,:] = mse_ada_svgd_small.squeeze()\n",
    "    mse_soul_all_small[i,:] = mse_soul_small.squeeze()\n",
    "\n",
    "    mse_latent_ada_pgd_all_small[i,:] = mse_latent_ada_pgd_small.squeeze()\n",
    "    mse_latent_ada_svgd_all_small[i,:] = mse_latent_ada_svgd_small.squeeze()\n",
    "    mse_latent_soul_all_small[i,:] = mse_latent_soul_small.squeeze()\n",
    "\n",
    "    # big LR\n",
    "    x_ada_pgd_big, th_ada_pgd_big = ada_pgd(model, data, X0, th0, K, latent_step_size=lr_big_ada_pgd, theta_step_size=lr_big_ada_pgd)\n",
    "    x_ada_svgd_big, th_ada_svgd_big = ada_svgd(model, data, X0, th0, K, theta_step_size=lr_big_ada_svgd, latent_step_size=lr_big_ada_svgd)\n",
    "    x_soul_big, th_soul_big = soul(model, data, X0, th0, K, theta_step_size=lr_big_soul, latent_step_size=lr_big_soul)\n",
    "\n",
    "    mse_ada_pgd_big = (th_ada_pgd_big - theta_star)**2\n",
    "    mse_ada_svgd_big = (th_ada_svgd_big - theta_star)**2\n",
    "    mse_soul_big = (th_soul_big - theta_star)**2\n",
    "\n",
    "    mse_latent_ada_pgd_big = ((x_ada_pgd_big.mean(axis=1) - posterior_mean_empirical[:,np.newaxis])**2).mean(axis=2)\n",
    "    mse_latent_ada_svgd_big = ((x_ada_svgd_big.mean(axis=1) - posterior_mean_empirical[:,np.newaxis])**2).mean(axis=2)\n",
    "    mse_latent_soul_big = ((x_soul_big.mean(axis=1) - posterior_mean_empirical[:,np.newaxis])**2).mean(axis=2)\n",
    "\n",
    "    x_ada_pgd_all_big[i, :, :, :], th_ada_pgd_all_big[i, :] = x_ada_pgd_big, th_ada_pgd_big.squeeze()\n",
    "    x_ada_svgd_all_big[i, :, :, :], th_ada_svgd_all_big[i, :] = x_ada_svgd_big, th_ada_svgd_big.squeeze()\n",
    "    x_soul_all_big[i, :, :, :], th_soul_all_big[i, :] = x_soul_big, th_soul_big.squeeze()\n",
    "\n",
    "    mse_ada_pgd_all_big[i,:] = mse_ada_pgd_big.squeeze()\n",
    "    mse_ada_svgd_all_big[i,:] = mse_ada_svgd_big.squeeze()\n",
    "    mse_soul_all_big[i,:] = mse_soul_big.squeeze()\n",
    "\n",
    "    mse_latent_ada_pgd_all_big[i,:] = mse_latent_ada_pgd_big.squeeze()\n",
    "    mse_latent_ada_svgd_all_big[i,:] = mse_latent_ada_svgd_big.squeeze()\n",
    "    mse_latent_soul_all_big[i,:] = mse_latent_soul_big.squeeze()\n",
    "\n",
    "    # coin \n",
    "    x_coin, th_coin = coin_svgd(model, data, X0, th0, K)\n",
    "    mse_coin = (th_coin - theta_star) ** 2\n",
    "    mse_latent_coin = ((x_coin.mean(axis=1) - posterior_mean_empirical[:,np.newaxis])**2).mean(axis=2)\n",
    "    x_coin_all[i, :, :, :], th_coin_all[i, :] = x_coin, th_coin.squeeze()\n",
    "    mse_coin_all[i, :] = mse_coin.squeeze()\n",
    "    mse_latent_coin_all[i,:] = mse_latent_coin.squeeze()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) MSE vs Iterations (Theta)  [Fig. 1(b), Fig. 6(b), Fig. 7(b), Fig. 9(b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot MSE vs iterations, using the optimal LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MSE (theta) vs iterations\n",
    "\n",
    "mse_coin = np.mean(mse_coin_all, axis=0)\n",
    "mse_ada_pgd = np.mean(mse_ada_pgd_all,axis=0) # mean over reps\n",
    "mse_ada_svgd = np.mean(mse_ada_svgd_all,axis=0) # mean over reps\n",
    "mse_soul = np.mean(mse_soul_all, axis=0) # mean over reps\n",
    "\n",
    "# compute confidence intervals\n",
    "\n",
    "data_ada_pgd = (mse_ada_pgd_all,)  # samples must be in a sequence\n",
    "res_ada_pgd = bootstrap(data_ada_pgd, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_ada_svgd = (mse_ada_svgd_all,)  # samples must be in a sequence\n",
    "res_ada_svgd = bootstrap(data_ada_svgd, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_soul = (mse_soul_all,)  # samples must be in a sequence\n",
    "res_soul = bootstrap(data_soul, np.mean, confidence_level=0.9, n_resamples=100)\n",
    "\n",
    "data_coin = (mse_coin_all,)  # samples must be in a sequence\n",
    "res_coin = bootstrap(data_coin, np.mean, confidence_level=0.9)\n",
    "\n",
    "plt.plot(range(K), mse_coin, color=\"C0\", zorder=10, label=\"Coin EM\", linewidth=3)\n",
    "plt.fill_between(range(K), res_coin.confidence_interval[0], res_coin.confidence_interval[1], color=\"C0\", alpha=0.2, linewidth=2)\n",
    "\n",
    "plt.plot(range(K), mse_ada_svgd, zorder=8, color=\"C2\", label=\"SVGD EM\", linestyle=\"dashed\", linewidth=3)\n",
    "plt.fill_between(range(K), res_ada_svgd.confidence_interval[0], res_ada_svgd.confidence_interval[1], color=\"C2\", alpha=0.2)\n",
    "\n",
    "plt.plot(range(K), mse_ada_pgd, color=\"C1\", label=\"PGD\", linestyle=\"dotted\", linewidth=3)\n",
    "plt.fill_between(range(K), res_ada_pgd.confidence_interval[0], res_ada_pgd.confidence_interval[1], color=\"C1\", alpha=0.2)\n",
    "\n",
    "plt.plot(range(K), mse_soul, color=\"C3\", label=\"SOUL\", linestyle='dashdot', linewidth=3)\n",
    "plt.fill_between(range(K), res_soul.confidence_interval[0], res_soul.confidence_interval[1], color=\"C3\", alpha=0.2)\n",
    "\n",
    "plt.legend(loc='upper right',prop={'size': 20})\n",
    "plt.xlabel('Iteration', fontsize=20)\n",
    "plt.ylabel(\"MSE (Theta Estimate)\", fontsize=20)\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(1e-16,10e1)\n",
    "plt.grid(color='whitesmoke')\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) MSE vs Iterations (Latent)  [Fig. 1(c), Fig. 6(c), Fig. 7(c), Fig. 9(c)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot the MSE of the posterior mean vs the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MSE (theta) vs iterations\n",
    "\n",
    "mse_latent_coin = np.mean(mse_latent_coin_all, axis=0)\n",
    "mse_latent_ada_pgd = np.mean(mse_latent_ada_pgd_all,axis=0) # mean over reps\n",
    "mse_latent_ada_svgd = np.mean(mse_latent_ada_svgd_all,axis=0) # mean over reps\n",
    "mse_latent_soul = np.mean(mse_latent_soul_all, axis=0) # mean over reps\n",
    "\n",
    "# compute confidence intervals\n",
    "\n",
    "data_ada_pgd = (mse_latent_ada_pgd_all,)  # samples must be in a sequence\n",
    "res_ada_pgd = bootstrap(data_ada_pgd, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_ada_svgd = (mse_latent_ada_svgd_all,)  # samples must be in a sequence\n",
    "res_ada_svgd = bootstrap(data_ada_svgd, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_soul = (mse_latent_soul_all,)  # samples must be in a sequence\n",
    "res_soul = bootstrap(data_soul, np.mean, confidence_level=0.9, n_resamples=100)\n",
    "\n",
    "data_coin = (mse_latent_coin_all,)  # samples must be in a sequence\n",
    "res_coin = bootstrap(data_coin, np.mean, confidence_level=0.9)\n",
    "\n",
    "plt.plot(range(K), mse_latent_coin, color=\"C0\", zorder=10, label=\"Coin EM\", linewidth=3)\n",
    "plt.fill_between(range(K), res_coin.confidence_interval[0], res_coin.confidence_interval[1], color=\"C0\", alpha=0.2, linewidth=2)\n",
    "\n",
    "plt.plot(range(K), mse_latent_ada_svgd, zorder=8, color=\"C2\", label=\"SVGD EM\", linestyle=\"dashed\", linewidth=3)\n",
    "plt.fill_between(range(K), res_ada_svgd.confidence_interval[0], res_ada_svgd.confidence_interval[1], color=\"C2\", alpha=0.2)\n",
    "\n",
    "plt.plot(range(K), mse_latent_ada_pgd, color=\"C1\", label=\"PGD\", linestyle=\"dotted\", linewidth=3)\n",
    "plt.fill_between(range(K), res_ada_pgd.confidence_interval[0], res_ada_pgd.confidence_interval[1], color=\"C1\", alpha=0.2)\n",
    "\n",
    "plt.plot(range(K), mse_latent_soul, color=\"C3\", label=\"SOUL\", linestyle='dashdot', linewidth=3)\n",
    "plt.fill_between(range(K), res_soul.confidence_interval[0], res_soul.confidence_interval[1], color=\"C3\", alpha=0.2)\n",
    "\n",
    "plt.legend(loc='upper right',prop={'size': 20})\n",
    "plt.xlabel('Iteration', fontsize=20)\n",
    "plt.ylabel(\"MSE (Posterior Mean)\", fontsize=20)\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(1e-15,10e1)\n",
    "plt.grid(color='whitesmoke')\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) SVGD EM vs Coin EM (Different LRs) [Fig. 8(a)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform a closer pairwise comparison between each method and Coin EM. Here, we take a closer look at SVGD EM. We plot the parameter MSE, the parameter estimates, and the posterior mean MSE, for three different choices of the learning rate: the optimal LR from before, a smaller LR, and a larger LR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MSE (theta) vs iterations\n",
    "\n",
    "mse_coin = np.mean(mse_coin_all, axis=0)\n",
    "mse_ada_svgd = np.mean(mse_ada_svgd_all,axis=0) # mean over reps\n",
    "mse_ada_svgd_small = np.mean(mse_ada_svgd_all_small,axis=0) # mean over reps\n",
    "mse_ada_svgd_big = np.mean(mse_ada_svgd_all_big,axis=0) # mean over reps\n",
    "\n",
    "# compute confidence intervals\n",
    "\n",
    "data_ada_svgd = (mse_ada_svgd_all,)  # samples must be in a sequence\n",
    "res_ada_svgd = bootstrap(data_ada_svgd, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_ada_svgd_small = (mse_ada_svgd_all_small,)  # samples must be in a sequence\n",
    "res_ada_svgd_small = bootstrap(data_ada_svgd_small, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_ada_svgd_big = (mse_ada_svgd_all_big,)  # samples must be in a sequence\n",
    "res_ada_svgd_big = bootstrap(data_ada_svgd_big, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_coin = (mse_coin_all,)  # samples must be in a sequence\n",
    "res_coin = bootstrap(data_coin, np.mean, confidence_level=0.9)\n",
    "\n",
    "plt.plot(range(K), mse_coin, color=\"C0\", zorder=10, label=\"Coin EM\", linewidth=3)\n",
    "plt.fill_between(range(K), res_coin.confidence_interval[0], res_coin.confidence_interval[1], color=\"C0\", alpha=0.2, linewidth=2)\n",
    "\n",
    "plt.plot(range(K), mse_ada_svgd, zorder=8, color=\"C2\", label=\"SVGD EM (Optimal LR)\", linestyle=\"dashed\", linewidth=3)\n",
    "plt.fill_between(range(K), res_ada_svgd.confidence_interval[0], res_ada_svgd.confidence_interval[1], color=\"C2\", alpha=0.2)\n",
    "\n",
    "plt.plot(range(K), mse_ada_svgd_small, color=\"C1\", label=\"SVGD EM (Small LR)\", linestyle=\"dotted\", linewidth=3)\n",
    "plt.fill_between(range(K), res_ada_svgd_small.confidence_interval[0], res_ada_svgd_small.confidence_interval[1], color=\"C1\", alpha=0.2)\n",
    "\n",
    "plt.plot(range(K), mse_ada_svgd_big, color=\"C3\", label=\"SVGD EM (Big LR)\", linestyle='dashdot', linewidth=3)\n",
    "plt.fill_between(range(K), res_ada_svgd_big.confidence_interval[0], res_ada_svgd_big.confidence_interval[1], color=\"C3\", alpha=0.2)\n",
    "\n",
    "plt.legend(loc='upper right',prop={'size': 16})\n",
    "plt.xlabel('Iteration', fontsize=16)\n",
    "plt.ylabel(\"MSE (Theta Estimate)\", fontsize=16)\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(1e-16,10e2)\n",
    "plt.grid(color='whitesmoke')\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot MSE (latent) vs iterations\n",
    "\n",
    "mse_latent_coin = np.mean(mse_latent_coin_all, axis=0)\n",
    "mse_latent_ada_svgd = np.mean(mse_latent_ada_svgd_all,axis=0) # mean over reps\n",
    "mse_latent_ada_svgd_small = np.mean(mse_latent_ada_svgd_all_small,axis=0) # mean over reps\n",
    "mse_latent_ada_svgd_big = np.mean(mse_latent_ada_svgd_all_big,axis=0) # mean over reps\n",
    "\n",
    "# compute confidence intervals\n",
    "\n",
    "data_ada_svgd = (mse_latent_ada_svgd_all,)  # samples must be in a sequence\n",
    "res_ada_svgd = bootstrap(data_ada_svgd, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_ada_svgd_small = (mse_latent_ada_svgd_all_small,)  # samples must be in a sequence\n",
    "res_ada_svgd_small = bootstrap(data_ada_svgd_small, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_ada_svgd_big = (mse_latent_ada_svgd_all_big,)  # samples must be in a sequence\n",
    "res_ada_svgd_big = bootstrap(data_ada_svgd_big, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_coin = (mse_latent_coin_all,)  # samples must be in a sequence\n",
    "res_coin = bootstrap(data_coin, np.mean, confidence_level=0.9)\n",
    "\n",
    "plt.plot(range(K), mse_latent_coin, color=\"C0\", zorder=10, label=\"Coin EM\", linewidth=3)\n",
    "plt.fill_between(range(K), res_coin.confidence_interval[0], res_coin.confidence_interval[1], color=\"C0\", alpha=0.2, linewidth=2)\n",
    "\n",
    "plt.plot(range(K), mse_latent_ada_svgd, zorder=8, color=\"C2\", label=\"SVGD EM (Optimal LR)\", linestyle=\"dashed\", linewidth=3)\n",
    "plt.fill_between(range(K), res_ada_svgd.confidence_interval[0], res_ada_svgd.confidence_interval[1], color=\"C2\", alpha=0.2)\n",
    "\n",
    "plt.plot(range(K), mse_latent_ada_svgd_small, color=\"C1\", label=\"SVGD EM (Small LR)\", linestyle=\"dotted\", linewidth=3)\n",
    "plt.fill_between(range(K), res_ada_svgd_small.confidence_interval[0], res_ada_svgd_small.confidence_interval[1], color=\"C1\", alpha=0.2)\n",
    "\n",
    "plt.plot(range(K), mse_latent_ada_svgd_big, color=\"C3\", label=\"SVGD EM (Big LR)\", linestyle='dashdot', linewidth=3)\n",
    "plt.fill_between(range(K), res_ada_svgd_big.confidence_interval[0], res_ada_svgd_big.confidence_interval[1], color=\"C3\", alpha=0.2)\n",
    "\n",
    "plt.legend(loc='upper right',prop={'size': 16})\n",
    "plt.xlabel('Iteration', fontsize=16)\n",
    "plt.ylabel(\"MSE (Latent Estimate)\", fontsize=16)\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(1e-16,10e2)\n",
    "plt.grid(color='whitesmoke')\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Plot theta vs iterations (illustrative run)\n",
    "which_rep = 6\n",
    "th_coin = th_coin_all[which_rep,:]\n",
    "th_svgd = th_ada_svgd_all[which_rep,:]\n",
    "th_svgd_small = th_ada_svgd_all_small[which_rep,:]\n",
    "th_svgd_big = th_ada_svgd_all_big[which_rep,:]\n",
    "\n",
    "# compute confidence intervals\n",
    "data_ada_svgd = (th_ada_svgd_all,)  # samples must be in a sequence\n",
    "res_ada_svgd = bootstrap(data_ada_svgd, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_ada_svgd_small = (th_ada_svgd_all_small,)  # samples must be in a sequence\n",
    "res_ada_svgd_small = bootstrap(data_ada_svgd_small, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_ada_svgd_big = (th_ada_svgd_all_big,)  # samples must be in a sequence\n",
    "res_ada_svgd_big = bootstrap(data_ada_svgd_big, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_coin = (th_coin_all,)  # samples must be in a sequence\n",
    "res_coin = bootstrap(data_coin, np.mean, confidence_level=0.9)\n",
    "\n",
    "plt.plot(range(K), th_coin, color=\"C0\", zorder=10, label=\"Coin EM\", linewidth=3)\n",
    "plt.plot(range(K), th_svgd, zorder=8, color=\"C2\", label=\"SVGD EM (Optimal LR)\", linestyle=\"dashed\", linewidth=3)\n",
    "plt.plot(range(K), th_svgd_small, color=\"C1\", label=\"SVGD EM (Small LR)\", linestyle=\"dotted\", linewidth=3)\n",
    "plt.plot(range(K), th_svgd_big, color=\"C3\", label=\"SVGD EM (Big LR)\", linestyle='dashdot', linewidth=3)\n",
    "\n",
    "plt.legend(loc='upper right',prop={'size': 16})\n",
    "plt.xlabel('Iteration', fontsize=16)\n",
    "plt.ylabel(\"Theta\", fontsize=16)\n",
    "plt.axhline(theta_star_all[which_rep], color=\"black\", linewidth=3, zorder=2)\n",
    "plt.xlim([-1,100]) # just plot first 100 iterations\n",
    "plt.ylim(-1,3)\n",
    "plt.grid(color='whitesmoke')\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) PGD vs Coin EM (Different LRs) [Fig. 8(b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the same plots for PGD. Once again, we plot the parameter MSE, the parameter estimates, and the posterior mean MSE, for three different choices of the learning rate: the optimal LR from before, a smaller LR, and a larger LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MSE (theta) vs iterations\n",
    "\n",
    "mse_coin = np.mean(mse_coin_all, axis=0)\n",
    "mse_ada_pgd = np.mean(mse_ada_pgd_all,axis=0) # mean over reps\n",
    "mse_ada_pgd_small = np.mean(mse_ada_pgd_all_small,axis=0) # mean over reps\n",
    "mse_ada_pgd_big = np.mean(mse_ada_pgd_all_big,axis=0) # mean over reps\n",
    "\n",
    "# compute confidence intervals\n",
    "\n",
    "data_ada_pgd = (mse_ada_pgd_all,)  # samples must be in a sequence\n",
    "res_ada_pgd = bootstrap(data_ada_pgd, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_ada_pgd_small = (mse_ada_pgd_all_small,)  # samples must be in a sequence\n",
    "res_ada_pgd_small = bootstrap(data_ada_pgd_small, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_ada_pgd_big = (mse_ada_pgd_all_big,)  # samples must be in a sequence\n",
    "res_ada_pgd_big = bootstrap(data_ada_pgd_big, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_coin = (mse_coin_all,)  # samples must be in a sequence\n",
    "res_coin = bootstrap(data_coin, np.mean, confidence_level=0.9)\n",
    "\n",
    "plt.plot(range(K), mse_coin, color=\"C0\", zorder=10, label=\"Coin EM\", linewidth=3)\n",
    "plt.fill_between(range(K), res_coin.confidence_interval[0], res_coin.confidence_interval[1], color=\"C0\", alpha=0.2, linewidth=2)\n",
    "\n",
    "plt.plot(range(K), mse_ada_pgd, zorder=8, color=\"C2\", label=\"PGD (Optimal LR)\", linestyle=\"dashed\", linewidth=3)\n",
    "plt.fill_between(range(K), res_ada_pgd.confidence_interval[0], res_ada_pgd.confidence_interval[1], color=\"C2\", alpha=0.2)\n",
    "\n",
    "plt.plot(range(K), mse_ada_pgd_small, color=\"C1\", label=\"PGD (Small LR)\", linestyle=\"dotted\", linewidth=3)\n",
    "plt.fill_between(range(K), res_ada_pgd_small.confidence_interval[0], res_ada_pgd_small.confidence_interval[1], color=\"C1\", alpha=0.2)\n",
    "\n",
    "plt.plot(range(K), mse_ada_pgd_big, color=\"C3\", label=\"PGD (Big LR)\", linestyle='dashdot', linewidth=3)\n",
    "plt.fill_between(range(K), res_ada_pgd_big.confidence_interval[0], res_ada_pgd_big.confidence_interval[1], color=\"C3\", alpha=0.2)\n",
    "\n",
    "plt.legend(loc='upper right',prop={'size': 16})\n",
    "plt.xlabel('Iteration', fontsize=16)\n",
    "plt.ylabel(\"MSE (Theta Estimate)\", fontsize=16)\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(1e-16,10e2)\n",
    "plt.grid(color='whitesmoke')\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot MSE (latent) vs iterations\n",
    "\n",
    "mse_latent_coin = np.mean(mse_latent_coin_all, axis=0)\n",
    "mse_latent_ada_pgd = np.mean(mse_latent_ada_pgd_all,axis=0) # mean over reps\n",
    "mse_latent_ada_pgd_small = np.mean(mse_latent_ada_pgd_all_small,axis=0) # mean over reps\n",
    "mse_latent_ada_pgd_big = np.mean(mse_latent_ada_pgd_all_big,axis=0) # mean over reps\n",
    "\n",
    "# compute confidence intervals\n",
    "\n",
    "data_ada_pgd = (mse_latent_ada_pgd_all,)  # samples must be in a sequence\n",
    "res_ada_pgd = bootstrap(data_ada_pgd, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_ada_pgd_small = (mse_latent_ada_pgd_all_small,)  # samples must be in a sequence\n",
    "res_ada_pgd_small = bootstrap(data_ada_pgd_small, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_ada_pgd_big = (mse_latent_ada_pgd_all_big,)  # samples must be in a sequence\n",
    "res_ada_pgd_big = bootstrap(data_ada_pgd_big, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_coin = (mse_latent_coin_all,)  # samples must be in a sequence\n",
    "res_coin = bootstrap(data_coin, np.mean, confidence_level=0.9)\n",
    "\n",
    "plt.plot(range(K), mse_latent_coin, color=\"C0\", zorder=10, label=\"Coin EM\", linewidth=3)\n",
    "plt.fill_between(range(K), res_coin.confidence_interval[0], res_coin.confidence_interval[1], color=\"C0\", alpha=0.2, linewidth=2)\n",
    "\n",
    "plt.plot(range(K), mse_latent_ada_pgd, zorder=8, color=\"C2\", label=\"PGD (Optimal LR)\", linestyle=\"dashed\", linewidth=3)\n",
    "plt.fill_between(range(K), res_ada_pgd.confidence_interval[0], res_ada_pgd.confidence_interval[1], color=\"C2\", alpha=0.2)\n",
    "\n",
    "plt.plot(range(K), mse_latent_ada_pgd_small, color=\"C1\", label=\"PGD (Small LR)\", linestyle=\"dotted\", linewidth=3)\n",
    "plt.fill_between(range(K), res_ada_pgd_small.confidence_interval[0], res_ada_pgd_small.confidence_interval[1], color=\"C1\", alpha=0.2)\n",
    "\n",
    "plt.plot(range(K), mse_latent_ada_pgd_big, color=\"C3\", label=\"PGD (Big LR)\", linestyle='dashdot', linewidth=3)\n",
    "plt.fill_between(range(K), res_ada_pgd_big.confidence_interval[0], res_ada_pgd_big.confidence_interval[1], color=\"C3\", alpha=0.2)\n",
    "\n",
    "plt.legend(loc='upper right',prop={'size': 16})\n",
    "plt.xlabel('Iteration', fontsize=16)\n",
    "plt.ylabel(\"MSE (Latent Estimate)\", fontsize=16)\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(1e-16,10e2)\n",
    "plt.grid(color='whitesmoke')\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot theta vs iterations (illustrative run)\n",
    "which_rep = 6\n",
    "th_coin = th_coin_all[which_rep,:]\n",
    "th_pgd = th_ada_pgd_all[which_rep,:]\n",
    "th_pgd_small = th_ada_pgd_all_small[which_rep,:]\n",
    "th_pgd_big = th_ada_pgd_all_big[which_rep,:]\n",
    "\n",
    "# compute confidence intervals\n",
    "\n",
    "data_ada_pgd = (th_ada_pgd_all,)  # samples must be in a sequence\n",
    "res_ada_pgd = bootstrap(data_ada_pgd, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_ada_pgd_small = (th_ada_pgd_all_small,)  # samples must be in a sequence\n",
    "res_ada_pgd_small = bootstrap(data_ada_pgd_small, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_ada_pgd_big = (th_ada_pgd_all_big,)  # samples must be in a sequence\n",
    "res_ada_pgd_big = bootstrap(data_ada_pgd_big, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_coin = (th_coin_all,)  # samples must be in a sequence\n",
    "res_coin = bootstrap(data_coin, np.mean, confidence_level=0.9)\n",
    "\n",
    "plt.plot(range(K), th_coin, color=\"C0\", zorder=10, label=\"Coin EM\", linewidth=3)\n",
    "plt.plot(range(K), th_pgd, zorder=8, color=\"C2\", label=\"PGD (Optimal LR)\", linestyle=\"dashed\", linewidth=3)\n",
    "plt.plot(range(K), th_pgd_small, color=\"C1\", label=\"PGD (Small LR)\", linestyle=\"dotted\", linewidth=3)\n",
    "plt.plot(range(K), th_pgd_big, color=\"C3\", label=\"PGD (Big LR)\", linestyle='dashdot', linewidth=3)\n",
    "\n",
    "plt.legend(loc='upper right',prop={'size': 16})\n",
    "plt.xlabel('Iteration', fontsize=16)\n",
    "plt.ylabel(\"Theta\", fontsize=16)\n",
    "plt.axhline(theta_star_all[which_rep], color=\"black\", linewidth=3, zorder=2)\n",
    "plt.xlim([-1,100]) # just plot first 100 iterations\n",
    "plt.ylim(-.5,2)\n",
    "plt.grid(color='whitesmoke')\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) SOUL vs Coin EM (Different LRs) [Fig. 8(c)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we recreate these plots for SOUL. Once again, we plot the parameter MSE, the parameter estimates, and the posterior mean MSE, for three different choices of the learning rate: the optimal LR from before, a smaller LR, and a larger LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MSE (theta) vs iterations\n",
    "\n",
    "mse_coin = np.mean(mse_coin_all, axis=0)\n",
    "mse_soul = np.mean(mse_soul_all,axis=0) # mean over reps\n",
    "mse_soul_small = np.mean(mse_soul_all_small,axis=0) # mean over reps\n",
    "mse_soul_big = np.mean(mse_soul_all_big,axis=0) # mean over reps\n",
    "\n",
    "# compute confidence intervals\n",
    "\n",
    "data_soul = (mse_soul_all,)  # samples must be in a sequence\n",
    "res_soul = bootstrap(data_soul, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_soul_small = (mse_soul_all_small,)  # samples must be in a sequence\n",
    "res_soul_small = bootstrap(data_soul_small, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_soul_big = (mse_soul_all_big,)  # samples must be in a sequence\n",
    "res_soul_big = bootstrap(data_soul_big, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_coin = (mse_coin_all,)  # samples must be in a sequence\n",
    "res_coin = bootstrap(data_coin, np.mean, confidence_level=0.9)\n",
    "\n",
    "plt.plot(range(K), mse_coin, color=\"C0\", zorder=10, label=\"Coin EM\", linewidth=3)\n",
    "plt.fill_between(range(K), res_coin.confidence_interval[0], res_coin.confidence_interval[1], color=\"C0\", alpha=0.2, linewidth=2)\n",
    "\n",
    "plt.plot(range(K), mse_soul, zorder=8, color=\"C2\", label=\"SOUL (Optimal LR)\", linestyle=\"dashed\", linewidth=3)\n",
    "plt.fill_between(range(K), res_soul.confidence_interval[0], res_soul.confidence_interval[1], color=\"C2\", alpha=0.2)\n",
    "\n",
    "plt.plot(range(K), mse_soul_small, color=\"C1\", label=\"SOUL (Small LR)\", linestyle=\"dotted\", linewidth=3)\n",
    "plt.fill_between(range(K), res_soul_small.confidence_interval[0], res_soul_small.confidence_interval[1], color=\"C1\", alpha=0.2)\n",
    "\n",
    "plt.plot(range(K), mse_soul_big, color=\"C3\", label=\"SOUL (Big LR)\", linestyle='dashdot', linewidth=3)\n",
    "plt.fill_between(range(K), res_soul_big.confidence_interval[0], res_soul_big.confidence_interval[1], color=\"C3\", alpha=0.2)\n",
    "\n",
    "plt.legend(loc='upper right',prop={'size': 16})\n",
    "plt.xlabel('Iteration', fontsize=16)\n",
    "plt.ylabel(\"MSE (Theta Estimate)\", fontsize=16)\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(1e-16,10e2)\n",
    "plt.grid(color='whitesmoke')\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Plot MSE (latent) vs iterations\n",
    "\n",
    "mse_latent_coin = np.mean(mse_latent_coin_all, axis=0)\n",
    "mse_latent_soul = np.mean(mse_latent_soul_all,axis=0) # mean over reps\n",
    "mse_latent_soul_small = np.mean(mse_latent_soul_all_small,axis=0) # mean over reps\n",
    "mse_latent_soul_big = np.mean(mse_latent_soul_all_big,axis=0) # mean over reps\n",
    "\n",
    "# compute confidence intervals\n",
    "\n",
    "data_soul = (mse_latent_soul_all,)  # samples must be in a sequence\n",
    "res_soul = bootstrap(data_soul, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_soul_small = (mse_latent_soul_all_small,)  # samples must be in a sequence\n",
    "res_soul_small = bootstrap(data_soul_small, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_soul_big = (mse_latent_soul_all_big,)  # samples must be in a sequence\n",
    "res_soul_big = bootstrap(data_soul_big, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_coin = (mse_latent_coin_all,)  # samples must be in a sequence\n",
    "res_coin = bootstrap(data_coin, np.mean, confidence_level=0.9)\n",
    "\n",
    "plt.plot(range(K), mse_latent_coin, color=\"C0\", zorder=10, label=\"Coin EM\", linewidth=3)\n",
    "plt.fill_between(range(K), res_coin.confidence_interval[0], res_coin.confidence_interval[1], color=\"C0\", alpha=0.2, linewidth=2)\n",
    "\n",
    "plt.plot(range(K), mse_latent_soul, zorder=8, color=\"C2\", label=\"SOUL (Optimal LR)\", linestyle=\"dashed\", linewidth=3)\n",
    "plt.fill_between(range(K), res_soul.confidence_interval[0], res_soul.confidence_interval[1], color=\"C2\", alpha=0.2)\n",
    "\n",
    "plt.plot(range(K), mse_latent_soul_small, color=\"C1\", label=\"SOUL (Small LR)\", linestyle=\"dotted\", linewidth=3)\n",
    "plt.fill_between(range(K), res_soul_small.confidence_interval[0], res_soul_small.confidence_interval[1], color=\"C1\", alpha=0.2)\n",
    "\n",
    "plt.plot(range(K), mse_latent_soul_big, color=\"C3\", label=\"PGD (Big LR)\", linestyle='dashdot', linewidth=3)\n",
    "plt.fill_between(range(K), res_soul_big.confidence_interval[0], res_soul_big.confidence_interval[1], color=\"C3\", alpha=0.2)\n",
    "\n",
    "plt.legend(loc='upper right',prop={'size': 16})\n",
    "plt.xlabel('Iteration', fontsize=16)\n",
    "plt.ylabel(\"MSE (Latent Estimate)\", fontsize=16)\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(1e-16,10e2)\n",
    "plt.grid(color='whitesmoke')\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Plot theta vs iterations (illustrative run)\n",
    "\n",
    "th_coin = th_coin_all[which_rep,:]\n",
    "th_soul = th_soul_all[which_rep,:]\n",
    "th_soul_small = th_soul_all_small[which_rep,:]\n",
    "th_soul_big = th_soul_all_big[which_rep,:]\n",
    "\n",
    "# compute confidence intervals\n",
    "\n",
    "data_soul = (th_ada_pgd_all,)  # samples must be in a sequence\n",
    "res_soul = bootstrap(data_soul, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_soul_small = (th_ada_pgd_all_small,)  # samples must be in a sequence\n",
    "res_soul_small = bootstrap(data_soul_small, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_ada_pgd_big = (th_ada_pgd_all_big,)  # samples must be in a sequence\n",
    "res_ada_pgd_big = bootstrap(data_soul_big, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_coin = (th_coin_all,)  # samples must be in a sequence\n",
    "res_coin = bootstrap(data_coin, np.mean, confidence_level=0.9)\n",
    "\n",
    "plt.plot(range(K), th_coin, color=\"C0\", zorder=10, label=\"Coin EM\", linewidth=3)\n",
    "plt.plot(range(K), th_soul, zorder=8, color=\"C2\", label=\"SOUL (Optimal LR)\", linestyle=\"dashed\", linewidth=3)\n",
    "plt.plot(range(K), th_soul_small, color=\"C1\", label=\"SOUL (Small LR)\", linestyle=\"dotted\", linewidth=3)\n",
    "plt.plot(range(K), th_soul_big, color=\"C3\", label=\"SOUL (Big LR)\", linestyle='dashdot', linewidth=3)\n",
    "\n",
    "plt.legend(loc='upper right',prop={'size': 16})\n",
    "plt.xlabel('Iteration', fontsize=16)\n",
    "plt.ylabel(\"Theta\", fontsize=16)\n",
    "plt.axhline(theta_star_all[which_rep], color=\"black\", linewidth=3, zorder=2)\n",
    "plt.xlim([-1,100]) # just plot first 100 iterations\n",
    "plt.ylim(-5,6)\n",
    "plt.grid(color='whitesmoke')\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Posterior Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider a single latent dimension. In this case, the optimal posterior variance is just given by 1/2. We start by running some repeats for Coin SVGD, EM SVGD, and PGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coinem.zoo import coin_svgd, ada_svgd, soul, pgd, ada_pgd, standard_svgd\n",
    "from coinem.marginal_zoo import marginal_coin_svgd, marginal_ada_svgd, marginal_soul, marginal_pgd\n",
    "    \n",
    "K = 5000  # Number of steps.\n",
    "\n",
    "Ny = 1\n",
    "Dy = 1\n",
    "key = jr.PRNGKey(42)\n",
    "theta = jnp.array([1.0])\n",
    "\n",
    "key_y, key_latent = jr.split(key)\n",
    "latent = theta + jr.normal(key_latent, (1, Dy))\n",
    "y = jr.normal(key_y, (Ny, Dy)) + latent\n",
    "\n",
    "data = Dataset(y=y)\n",
    "\n",
    "reps = 10\n",
    "N_list = [2, 5, 10, 20, 50, 100]\n",
    "    \n",
    "x_coin_all_list, th_coin_all_list = [], []\n",
    "x_svgd_all_list, th_svgd_all_list = [], []\n",
    "x_pgd_all_list, th_pgd_all_list = [], []\n",
    "\n",
    "for i, N in enumerate(N_list):\n",
    "\n",
    "    print(\"N value: \" + str(i+1) + \"/\" + str(len(N_list)))\n",
    "\n",
    "    x_coin_all, th_coin_all = np.zeros((reps, K, N, Dy)), np.zeros((reps, K))\n",
    "    x_svgd_all, th_svgd_all = np.zeros((reps, K, N, Dy)), np.zeros((reps, K))\n",
    "    x_pgd_all, th_pgd_all = np.zeros((reps, K, N, Dy)), np.zeros((reps, K))\n",
    "\n",
    "    for j, rep in enumerate(range(reps)):\n",
    "\n",
    "        print(\"Repetition: \" + str(j+1) + \"/\" + str(reps))\n",
    "\n",
    "        key, subkey = jr.split(key)\n",
    "        th0 = 1 + jr.normal(subkey, (1,)) # initial parameter guess.\n",
    "        X0 = jr.normal(subkey, (N, Dy))  # initial particle cloud.\n",
    "\n",
    "        model = HierarchicalModel()\n",
    "\n",
    "        # Run methods\n",
    "        x_coin, th_coin = coin_svgd(model, data, X0, th0, K) \n",
    "        x_svgd, th_svgd = ada_svgd(model, data, X0, th0, K, latent_step_size=1, theta_step_size=1)\n",
    "        x_pgd, th_pgd = ada_pgd(model, data, X0, th0, K)\n",
    "\n",
    "        x_coin_all[j, :, :], th_coin_all[j, :] = x_coin, th_coin.squeeze()\n",
    "        x_svgd_all[j, :, :], th_svgd_all[j, :] = x_svgd, th_svgd.squeeze()\n",
    "        x_pgd_all[j, :, :], th_pgd_all[j, :] = x_pgd, th_pgd.squeeze()\n",
    "\n",
    "    x_coin_all_list.append(x_coin_all)\n",
    "    th_coin_all_list.append(th_coin_all)\n",
    "\n",
    "    x_svgd_all_list.append(x_svgd_all)\n",
    "    th_svgd_all_list.append(th_svgd_all)\n",
    "\n",
    "    x_pgd_all_list.append(x_pgd_all)\n",
    "    th_pgd_all_list.append(th_pgd_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)(i) Posterior Variance vs Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results. First we'll compute the variance by averaging over time and over the particles. It seems coin tends to underestimate the variance, for smaller numbers of particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "    \n",
    "fig1, ax1 = plt.subplots(1, figsize=(5,4))\n",
    "fig2, ax2 = plt.subplots(1, figsize=(5,4))\n",
    "fig3, ax3 = plt.subplots(1, figsize=(5,4))\n",
    "\n",
    "for i, N in enumerate(N_list):\n",
    "    \n",
    "    x_coin_all = x_coin_all_list[i]\n",
    "    x_svgd_all = x_svgd_all_list[i]\n",
    "    x_pgd_all = x_pgd_all_list[i]    \n",
    "    \n",
    "    #mu1_coin = x_coin_all.mean(axis=2) ** 2 # E[x]^2, expectation over particles\n",
    "    mu1_coin = x_coin_all.mean(axis=2)\n",
    "    mu2_coin = (x_coin_all ** 2).mean(axis=2) # E[x^2], expectation over particles\n",
    "    #var_coin = (mu2_coin - mu1_coin).squeeze() # Var(x)\n",
    "    var_coin = ((jnp.cumsum(mu2_coin.squeeze(), axis=1)/jnp.arange(1, K + 1)\n",
    "            - (jnp.cumsum(mu1_coin.squeeze(), axis=1)/jnp.arange(1, K + 1)) ** 2))\n",
    "    var_coin_mean = var_coin.mean(axis=0) # average over reps\n",
    "    var_coin_std = var_coin.std(axis=0) # std over reps\n",
    "    \n",
    "    #mu1_svgd = x_svgd_all.mean(axis=2) ** 2\n",
    "    mu1_svgd = x_svgd_all.mean(axis=2) \n",
    "    mu2_svgd = (x_svgd_all ** 2).mean(axis=2)\n",
    "    #var_svgd = (mu2_svgd - mu1_svgd).squeeze() # Var(x)\n",
    "    var_svgd = ((jnp.cumsum(mu2_svgd.squeeze(), axis=1)/jnp.arange(1, K + 1)\n",
    "            - (jnp.cumsum(mu1_svgd.squeeze(), axis=1)/jnp.arange(1, K + 1)) ** 2))\n",
    "    var_svgd_mean = var_svgd.mean(axis=0)\n",
    "    var_svgd_std = var_svgd.std(axis=0)\n",
    "    \n",
    "    mu1_pgd = x_pgd_all.mean(axis=2)\n",
    "    mu2_pgd = (x_pgd_all ** 2).mean(axis=2)\n",
    "    var_pgd = ((jnp.cumsum(mu2_pgd.squeeze(), axis=1)/jnp.arange(1, K + 1)\n",
    "            - (jnp.cumsum(mu1_pgd.squeeze(), axis=1)/jnp.arange(1, K + 1)) ** 2))\n",
    "    var_pgd_mean = var_pgd.mean(axis=0)\n",
    "    var_pgd_std = var_pgd.std(axis=0)\n",
    "\n",
    "    ax1.plot(range(K), var_coin_mean, label='N = {}'.format(N), color=\"C\" + str(i), linewidth=3)\n",
    "    ax1.fill_between(range(K), var_coin_mean - var_coin_std, var_coin_mean + var_coin_std, color=\"C\" + str(i), alpha=0.1)\n",
    "    ax2.plot(range(K), var_svgd_mean, label='N = {}'.format(N), color=\"C\" + str(i), linewidth=3)\n",
    "    ax2.fill_between(range(K), var_svgd_mean - var_svgd_std, var_svgd_mean + var_svgd_std, color=\"C\" + str(i), alpha=0.1)\n",
    "    ax3.plot(range(K), var_pgd_mean, label='N = {}'.format(N), color=\"C\" + str(i), linewidth=3)\n",
    "    ax3.fill_between(range(K), var_pgd_mean - var_pgd_std, var_pgd_mean + var_pgd_std, color=\"C\" + str(i), alpha=0.1)\n",
    "\n",
    "ax1.grid(color=\"whitesmoke\")\n",
    "ax2.grid(color='whitesmoke')\n",
    "ax3.grid(color='whitesmoke')\n",
    "ax1.set_xlabel(\"Iterations\", fontsize=16)\n",
    "ax2.set_xlabel(\"Iterations\", fontsize=16)\n",
    "ax3.set_xlabel(\"Iterations\", fontsize=16)\n",
    "ax1.set_ylabel(\"Posterior Variance\", fontsize=16)\n",
    "ax2.set_ylabel(\"Posterior Variance\", fontsize=16)\n",
    "ax3.set_ylabel(\"Posterior Variance\", fontsize=16)\n",
    "ax1.tick_params(axis='both', labelsize=16)\n",
    "ax2.tick_params(axis='both', labelsize=16)\n",
    "ax3.tick_params(axis='both', labelsize=16)\n",
    "ax1.set_ylim(-0.1,1.4)\n",
    "ax2.set_ylim(-0.1,1.4)\n",
    "ax3.set_ylim(-0.1,1.4)\n",
    "ax1.hlines(0.5, 0, K, label='Optimal variance', linestyles='dashed', color='black', linewidth=3)\n",
    "ax2.hlines(0.5, 0, K, label='Optimal variance', linestyles='dashed', color='black', linewidth=3)\n",
    "ax3.hlines(0.5, 0, K, label='Optimal variance', linestyles='dashed', color='black', linewidth=3)\n",
    "ax1.legend(loc='upper right', prop={'size': 11})\n",
    "ax2.legend(loc='upper right', prop={'size': 11})\n",
    "ax3.legend(loc='upper right', prop={'size': 11})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Posterior Variance vs Iterations [Fig. 2(a) - 2(b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repeat the same calculations, but this time not averaging over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "    \n",
    "fig1, ax1 = plt.subplots(1, figsize=(5,4))\n",
    "fig2, ax2 = plt.subplots(1, figsize=(5,4))\n",
    "fig3, ax3 = plt.subplots(1, figsize=(5,4))\n",
    "fig4, ax4 = plt.subplots(1, figsize=(5,4))\n",
    "\n",
    "for i, N in enumerate(N_list):\n",
    "    \n",
    "    x_coin_all = x_coin_all_list[i]\n",
    "    x_svgd_all = x_svgd_all_list[i]\n",
    "    x_pgd_all = x_pgd_all_list[i]\n",
    "\n",
    "    mu1_coin = x_coin_all.mean(axis=2) ** 2 # E[x]^2, expectation over particles\n",
    "    mu2_coin = (x_coin_all ** 2).mean(axis=2) # E[x^2], expectation over particles\n",
    "    var_coin = (mu2_coin - mu1_coin).squeeze() # Var(x)\n",
    "    var_coin_mean = var_coin.mean(axis=0) # average over reps\n",
    "    var_coin_std = var_coin.std(axis=0) # std over reps\n",
    "    \n",
    "    mu1_svgd = x_svgd_all.mean(axis=2) ** 2\n",
    "    mu2_svgd = (x_svgd_all ** 2).mean(axis=2)\n",
    "    var_svgd = (mu2_svgd - mu1_svgd).squeeze() # Var(x)\n",
    "    var_svgd_mean = var_svgd.mean(axis=0)\n",
    "    var_svgd_std = var_svgd.std(axis=0)\n",
    "    \n",
    "    mu1_pgd = x_pgd_all.mean(axis=2) ** 2\n",
    "    mu2_pgd = (x_pgd_all ** 2).mean(axis=2)\n",
    "    var_pgd = (mu2_pgd - mu1_pgd).squeeze() # Var(x)\n",
    "    var_pgd_mean = var_pgd.mean(axis=0)\n",
    "    var_pgd_std = var_pgd.std(axis=0)\n",
    "\n",
    "    K_plot = 500\n",
    "    ax1.plot(range(K_plot), var_coin_mean[:K_plot], label='N = {}'.format(N), color=\"C\" + str(i), linewidth=3)\n",
    "    ax1.fill_between(range(K_plot), var_coin_mean[:K_plot] - var_coin_std[:K_plot], var_coin_mean[:K_plot] + var_coin_std[:K_plot], color=\"C\" + str(i), alpha=0.1)\n",
    "    ax2.plot(range(K_plot), var_svgd_mean[:K_plot], label='N = {}'.format(N), color=\"C\" + str(i), linewidth=3)\n",
    "    ax2.fill_between(range(K_plot), var_svgd_mean[:K_plot] - var_svgd_std[:K_plot], var_svgd_mean[:K_plot] + var_svgd_std[:K_plot], color=\"C\" + str(i), alpha=0.1)\n",
    "    ax3.plot(range(K_plot), var_pgd_mean[:K_plot], label='N = {}'.format(N), color=\"C\" + str(i), linewidth=3)\n",
    "    ax3.fill_between(range(K_plot), var_pgd_mean[:K_plot] - var_pgd_std[:K_plot], var_pgd_mean[:K_plot] + var_pgd_std[:K_plot], color=\"C\" + str(i), alpha=0.1)\n",
    "    ax4.plot(range(K_plot), np.cumsum(var_pgd_mean[:K_plot])/np.arange(1,K_plot+1), label='N = {}'.format(N), color=\"C\" + str(i), linewidth=3)\n",
    "    ax4.fill_between(range(K_plot), var_pgd_mean[:K_plot] - var_pgd_std[:K_plot], var_pgd_mean[:K_plot] + var_pgd_std[:K_plot], color=\"C\" + str(i), alpha=0.1)\n",
    "\n",
    "ax1.grid(color=\"whitesmoke\")\n",
    "ax2.grid(color='whitesmoke')\n",
    "ax3.grid(color='whitesmoke')\n",
    "ax4.grid(color='whitesmoke')\n",
    "ax1.set_xlabel(\"Iterations\", fontsize=16)\n",
    "ax2.set_xlabel(\"Iterations\", fontsize=16)\n",
    "ax3.set_xlabel(\"Iterations\", fontsize=16)\n",
    "ax4.set_xlabel(\"Iterations\", fontsize=16)\n",
    "ax1.set_ylabel(\"Posterior Variance\", fontsize=16)\n",
    "ax2.set_ylabel(\"Posterior Variance\", fontsize=16)\n",
    "ax3.set_ylabel(\"Posterior Variance\", fontsize=16)\n",
    "ax4.set_ylabel(\"Posterior Variance\", fontsize=16)\n",
    "ax1.tick_params(axis='both', labelsize=16)\n",
    "ax2.tick_params(axis='both', labelsize=16)\n",
    "ax3.tick_params(axis='both', labelsize=16)\n",
    "ax4.tick_params(axis='both', labelsize=16)\n",
    "ax1.set_ylim(-0.1,1.4)\n",
    "ax2.set_ylim(-0.1,1.4)\n",
    "ax3.set_ylim(-0.1,1.4)\n",
    "ax4.set_ylim(-0.1,1.4)\n",
    "ax1.hlines(0.5, 0, K_plot, label='Optimal variance', linestyles='dashed', color='black', linewidth=3)\n",
    "ax2.hlines(0.5, 0, K_plot, label='Optimal variance', linestyles='dashed', color='black', linewidth=3)\n",
    "ax3.hlines(0.5, 0, K_plot, label='Optimal variance', linestyles='dashed', color='black', linewidth=3)\n",
    "ax4.hlines(0.5, 0, K_plot, label='Optimal variance', linestyles='dashed', color='black', linewidth=3)\n",
    "ax1.legend(loc='upper right', prop={'size': 11})\n",
    "ax2.legend(loc='upper right', prop={'size': 11})\n",
    "ax3.legend(loc='upper right', prop={'size': 11})\n",
    "ax4.legend(loc='upper right', prop={'size': 11})\n",
    "\n",
    "fig1.savefig(fig_dir + \"/\" + \"toy_\" + \"var_exp_coin_non_average.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "fig2.savefig(fig_dir + \"/\" + \"toy_\" + \"var_exp_svgd_non_average.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "fig3.savefig(fig_dir + \"/\" + \"toy_\" + \"var_exp_pgd_non_average.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "fig4.savefig(fig_dir + \"/\" + \"toy_\" + \"var_exp_pgd_non_average_smooth.pdf\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) MSE of Posterior Variance vs Learning Rate [Fig. 2(c)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's take a look at what happens to the posterior variance estimates obtained by the various methods as we change the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coinem.zoo import coin_svgd, ada_svgd, soul, pgd, ada_pgd, standard_svgd\n",
    "from coinem.marginal_zoo import marginal_coin_svgd, marginal_ada_svgd, marginal_soul, marginal_pgd\n",
    "    \n",
    "key = jr.PRNGKey(42)\n",
    "K = 250  # Number of steps.\n",
    "\n",
    "Ny = 1\n",
    "Dy = 1\n",
    "theta = jnp.array([1.0])\n",
    "\n",
    "key_y, key_latent = jr.split(key)\n",
    "latent = theta + jr.normal(key_latent, (1, Dy))\n",
    "y = jr.normal(key_y, (Ny, Dy)) + latent\n",
    "\n",
    "data = Dataset(y=y)\n",
    "\n",
    "N = 50 # num particles\n",
    "reps = 5 # num repeats\n",
    "\n",
    "lr_list = np.logspace(-8,3,20) # step sizes\n",
    "\n",
    "x_coin_all_list_lr, th_coin_all_list_lr = [], []\n",
    "x_svgd_all_list_lr, th_svgd_all_list_lr = [], []\n",
    "x_pgd_all_list_lr, th_pgd_all_list_lr = [], []\n",
    "x_soul_all_list_lr, th_soul_all_list_lr = [], []\n",
    "\n",
    "for i, lr in enumerate(lr_list):\n",
    "\n",
    "    print(\"LR value: \" + str(i+1) + \"/\" + str(len(lr_list)))\n",
    "\n",
    "    x_coin_all, th_coin_all = np.zeros((reps, K, N, Dy)), np.zeros((reps, K))\n",
    "    x_svgd_all, th_svgd_all = np.zeros((reps, K, N, Dy)), np.zeros((reps, K))\n",
    "    x_pgd_all, th_pgd_all = np.zeros((reps, K, N, Dy)), np.zeros((reps, K))\n",
    "    x_soul_all, th_soul_all = np.zeros((reps, K, N, Dy)), np.zeros((reps, K))\n",
    "\n",
    "    for j, rep in enumerate(range(reps)):\n",
    "\n",
    "        print(\"Repetition: \" + str(j+1) + \"/\" + str(reps))\n",
    "\n",
    "        key, subkey = jr.split(key)\n",
    "        th0 = 1 + jr.normal(subkey, (1,)) # initial parameter guess.\n",
    "        X0 = jr.normal(subkey, (N, Dy))  # initial particle cloud.\n",
    "\n",
    "        model = HierarchicalModel()\n",
    "\n",
    "        # Run methods\n",
    "        x_coin, th_coin = coin_svgd(model, data, X0, th0, K) \n",
    "        x_svgd, th_svgd = ada_svgd(model, data, X0, th0, K, latent_step_size=lr, theta_step_size=lr)\n",
    "        x_pgd, th_pgd = ada_pgd(model, data, X0, th0, K, latent_step_size=lr, theta_step_size=lr)\n",
    "        x_soul, th_soul = soul(model, data, X0, th0, K, latent_step_size=lr, theta_step_size=lr)\n",
    "\n",
    "        x_coin_all[j, :, :], th_coin_all[j, :] = x_coin, th_coin.squeeze()\n",
    "        x_svgd_all[j, :, :], th_svgd_all[j, :] = x_svgd, th_svgd.squeeze()\n",
    "        x_pgd_all[j, :, :], th_pgd_all[j, :] = x_pgd, th_pgd.squeeze()\n",
    "        x_soul_all[j, :, :], th_soul_all[j, :] = x_soul, th_soul.squeeze()\n",
    "\n",
    "    x_coin_all_list_lr.append(x_coin_all)\n",
    "    th_coin_all_list_lr.append(th_coin_all)\n",
    "\n",
    "    x_svgd_all_list_lr.append(x_svgd_all)\n",
    "    th_svgd_all_list_lr.append(th_svgd_all)\n",
    "\n",
    "    x_pgd_all_list_lr.append(x_pgd_all)\n",
    "    th_pgd_all_list_lr.append(th_pgd_all)\n",
    "\n",
    "    x_soul_all_list_lr.append(x_soul_all)\n",
    "    th_soul_all_list_lr.append(th_soul_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll process and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(1, figsize=(5,4))\n",
    "\n",
    "all_coin_var = np.zeros((reps, len(lr_list)))\n",
    "all_svgd_var = np.zeros((reps, len(lr_list)))\n",
    "all_pgd_var = np.zeros((reps, len(lr_list)))\n",
    "all_soul_var = np.zeros((reps, len(lr_list)))\n",
    "\n",
    "for i, N in enumerate(lr_list):\n",
    "    \n",
    "    x_coin_all = x_coin_all_list_lr[i]\n",
    "    x_svgd_all = x_svgd_all_list_lr[i]\n",
    "    x_pgd_all = x_pgd_all_list_lr[i]\n",
    "    x_soul_all = x_soul_all_list_lr[i]\n",
    "    \n",
    "    mu1_coin = x_coin_all.mean(axis=2) ** 2 # E[x]^2, expectation over particles\n",
    "    mu2_coin = (x_coin_all ** 2).mean(axis=2) # E[x^2], expectation over particles\n",
    "    var_coin = (mu2_coin - mu1_coin).squeeze() # Var(x)\n",
    "    var_coin_final = var_coin[:,-1]\n",
    "    \n",
    "    mu1_svgd = x_svgd_all.mean(axis=2) ** 2\n",
    "    mu2_svgd = (x_svgd_all ** 2).mean(axis=2)\n",
    "    var_svgd = (mu2_svgd - mu1_svgd).squeeze() # Var(x)\n",
    "    var_svgd_final = var_svgd[:,-1]\n",
    "    \n",
    "    mu1_pgd = x_pgd_all.mean(axis=2) ** 2\n",
    "    mu2_pgd = (x_pgd_all ** 2).mean(axis=2)\n",
    "    var_pgd = (mu2_pgd - mu1_pgd).squeeze() # Var(x)\n",
    "    var_pgd_final = np.mean(var_pgd, axis=1)\n",
    "    \n",
    "    mu1_soul = x_soul_all.mean(axis=2) ** 2\n",
    "    mu2_soul = (x_soul_all ** 2).mean(axis=2)\n",
    "    var_soul = (mu2_soul - mu1_soul).squeeze() # Var(x)\n",
    "    var_soul_final = np.mean(var_soul, axis=1)\n",
    "    \n",
    "    all_coin_var[:,i] = var_coin_final\n",
    "    all_svgd_var[:,i] = var_svgd_final\n",
    "    all_pgd_var[:,i] = var_pgd_final\n",
    "    all_soul_var[:,i] = var_soul_final\n",
    "\n",
    "mse_coin = ((all_coin_var - 0.5) ** 2).mean(axis=0)\n",
    "mse_svgd = (all_svgd_var - 0.5) ** 2\n",
    "mse_pgd = (all_pgd_var - 0.5) ** 2\n",
    "mse_soul = (all_soul_var - 0.5) ** 2\n",
    "\n",
    "data_coin = (mse_coin,)  # samples must be in a sequence\n",
    "res_coin = bootstrap(data_coin, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_svgd = (mse_svgd,)  # samples must be in a sequence\n",
    "res_svgd = bootstrap(data_svgd, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_pgd = (mse_pgd,)  # samples must be in a sequence\n",
    "res_pgd = bootstrap(data_pgd, np.mean, confidence_level=0.9)\n",
    "\n",
    "data_soul = (mse_soul,)  # samples must be in a sequence\n",
    "res_soul = bootstrap(data_soul, np.mean, confidence_level=0.9)\n",
    "\n",
    "plt.axhline(np.mean(mse_coin), color=\"C0\", label=\"Coin EM\", linewidth=3)\n",
    "plt.fill_between(lr_list, res_coin.confidence_interval[0], res_coin.confidence_interval[1], color=\"C0\", alpha=0.2)\n",
    "plt.plot(lr_list, np.mean(mse_svgd,axis=0), color=\"C2\", label=\"SVGD EM\", linewidth=3, linestyle='dashed')\n",
    "plt.fill_between(lr_list, res_svgd.confidence_interval[0], res_svgd.confidence_interval[1], color=\"C2\", alpha=0.2)\n",
    "plt.plot(lr_list, np.mean(mse_pgd,axis=0),  color=\"C1\",label='PGD', linewidth=3, linestyle='dotted')\n",
    "plt.fill_between(lr_list, res_pgd.confidence_interval[0], res_pgd.confidence_interval[1], color=\"C1\", alpha=0.2)\n",
    "plt.plot(lr_list, np.mean(mse_soul,axis=0), color=\"C3\",label='SOUL', linewidth=3, linestyle='dashdot')\n",
    "plt.fill_between(lr_list, res_soul.confidence_interval[0], res_soul.confidence_interval[1], color=\"C3\", alpha=0.2)\n",
    "plt.legend(prop={'size': 16})\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Learning Rate\", fontsize=16)\n",
    "plt.ylabel(\"MSE (Posterior Variance)\", fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.grid(color='whitesmoke')\n",
    "ax1.set_axisbelow(True)\n",
    "plt.ylim(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gcsX2yOfakVA",
    "PXeTOIZy-tZA",
    "ElXW-4d9B9nc"
   ],
   "include_colab_link": true,
   "name": "Toy Hierarchical Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_m1",
   "language": "python",
   "name": "venv_m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
